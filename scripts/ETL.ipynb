{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06753bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created and connected to MinIO!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# =============================================================================\n",
    "# SPARK SESSION INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"COMS_Project_Docker\") \\\n",
    "    .master(\"spark://coms-spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://coms-minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio_user\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio_password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.6,\"\n",
    "        \"org.apache.hadoop:hadoop-client:3.3.6,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.367\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created and connected to MinIO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c588d716-7030-4e25-b3e4-97b593f76932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "import os\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "SMTP_SERVER = 'smtp.gmail.com'\n",
    "SMTP_PORT = 587\n",
    "SENDER_EMAIL = 'tindangbao5603@gmail.com'\n",
    "SENDER_PASSWORD = 'imboqodkokyeyjhj'\n",
    "\n",
    "def send_email(recipient_email, subject, body):\n",
    "    \"\"\"A generic function to send an email using SMTP.\"\"\"\n",
    "    if not SENDER_EMAIL or not SENDER_PASSWORD:\n",
    "        print(\"ERROR: SENDER_EMAIL or SENDER_PASSWORD environment variables not set.\")\n",
    "        print(\"Email not sent.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # Create the email message\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = SENDER_EMAIL\n",
    "        msg['To'] = recipient_email\n",
    "        msg['Subject'] = subject\n",
    "        msg.attach(MIMEText(body, 'html')) # Use 'html' for rich text formatting\n",
    "\n",
    "        # Connect to the SMTP server and send the email\n",
    "        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.starttls()  # Secure the connection\n",
    "            server.login(SENDER_EMAIL, SENDER_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "            print(f\"Successfully sent email to {recipient_email}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email to {recipient_email}. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def send_delayed_payment_alert(customer_name, customer_email, order_id, order_date, order_amount, currency=\"USD\"):\n",
    "    \"\"\"Sends a payment reminder to a customer.\"\"\"\n",
    "    subject = f\"Action Required: Your Payment for Order #{order_id} is Overdue\"\n",
    "    \n",
    "    # Using an HTML template for better formatting\n",
    "    body = f\"\"\"\n",
    "    <html>\n",
    "    <body>\n",
    "        <p>Hi <strong>{customer_name}</strong>,</p>\n",
    "        <p>We're writing to remind you that the payment for your recent order is now overdue.</p>\n",
    "        <p>Here are the details of the transaction:</p>\n",
    "        <ul>\n",
    "            <li><strong>Order ID:</strong> {order_id}</li>\n",
    "            <li><strong>Order Date:</strong> {order_date}</li>\n",
    "            <li><strong>Total Amount:</strong> {order_amount} {currency}</li>\n",
    "        </ul>\n",
    "        <p>To avoid any disruption or cancellation of your order, please complete your payment as soon as possible. You can make the payment by clicking the secure link below:</p>\n",
    "        <p><a href=\"https://yourcompany.com/pay/{order_id}\" style=\"background-color: #007bff; color: white; padding: 10px 15px; text-decoration: none; border-radius: 5px;\">Pay Now</a></p>\n",
    "        <p>If you've already made the payment, please disregard this email. If you're facing any issues or have questions about your order, please don't hesitate to contact our support team.</p>\n",
    "        <p>Thank you,<br>The COMS Team</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    send_email(customer_email, subject, body)\n",
    "\n",
    "def send_etl_error_alert(tech_email, pipeline_stage, table_name, error_message):\n",
    "    \"\"\"Sends a critical failure alert to the tech team.\"\"\"\n",
    "    subject = f\"CRITICAL: ETL Job Failed - {pipeline_stage} - {table_name}\"\n",
    "    \n",
    "    body = f\"\"\"\n",
    "    <html>\n",
    "    <body>\n",
    "        <h2>ETL Process Alert: FAILURE</h2>\n",
    "        <p>An error occurred during the data processing pipeline. Immediate investigation is required.</p>\n",
    "        <h3>Details:</h3>\n",
    "        <ul>\n",
    "            <li><strong>Timestamp (UTC):</strong> {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}</li>\n",
    "            <li><strong>Pipeline Stage:</strong> {pipeline_stage}</li>\n",
    "            <li><strong>Table/Process:</strong> {table_name}</li>\n",
    "        </ul>\n",
    "        <h3>Error Message:</h3>\n",
    "        <pre style=\"background-color: #f8d7da; color: #721c24; padding: 10px; border: 1px solid #f5c6cb; border-radius: 5px;\">{error_message}</pre>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    send_email(tech_email, subject, body)\n",
    "\n",
    "def send_etl_success_notification(tech_email, pipeline_stage, output_path):\n",
    "    \"\"\"Sends a success confirmation to the tech team.\"\"\"\n",
    "    subject = f\"SUCCESS: ETL Job Completed - {pipeline_stage}\"\n",
    "    \n",
    "    body = f\"\"\"\n",
    "    <html>\n",
    "    <body>\n",
    "        <h2>ETL Process Alert: SUCCESS</h2>\n",
    "        <p>The following process - <strong>{pipeline_stage}</strong> - has been processed and loaded successfully.</p>\n",
    "        <h3>Details:</h3>\n",
    "        <ul>\n",
    "            <li><strong>Timestamp (UTC):</strong> {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}</li>\n",
    "            <li><strong>Output Path:</strong> {output_path}</li>\n",
    "        </ul>\n",
    "        <p>No action is required. This is an automated success notification.</p>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    send_email(tech_email, subject, body)\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Running email script examples...\")\n",
    "    \n",
    "#     # Example 1: Alert a customer about a delayed payment\n",
    "#     # In your real code, you would get this data from your 'delayed_payment_alerts' DataFrame\n",
    "#     send_delayed_payment_alert(\n",
    "#         customer_name=\"John Doe\",\n",
    "#         customer_email=\"tindang05062003@gmail.com\",\n",
    "#         order_id=\"ORD-2025-101\",\n",
    "#         order_date=\"2025-08-20\",\n",
    "#         order_amount=199.99\n",
    "#     )\n",
    "\n",
    "#     # Example 2: Alert the tech team about a failed ETL job\n",
    "#     # In your real code, this would be in a try...except block\n",
    "#     error_details = \"Py4JJavaError: An error occurred while calling o123.showString.\\\\n...\\\\nCaused by: org.apache.spark.SparkException: Job aborted.\"\n",
    "#     send_etl_error_alert(\n",
    "#         pipeline_stage=\"Processed to Curated\",\n",
    "#         table_name=\"daily_sales_aggregates\",\n",
    "#         error_message=error_details,\n",
    "#         log_path=\"s3a://logs/etl/processed-curated/2025-08-26.log\"\n",
    "#     )\n",
    "\n",
    "#     # Example 3: Notify the tech team of a successful job\n",
    "#     # In your real code, this would be at the end of a successful 'process_table' function call\n",
    "#     send_etl_success_notification(\n",
    "#         pipeline_stage=\"Raw to Processed\",\n",
    "#         table_name=\"customers_csv\",\n",
    "#         records_processed=1500,\n",
    "#         output_path=\"s3a://processed/customers_csv\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ea5ba-88e4-4884-ac39-5aebea8c82bb",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Verification\n",
    "Raw input files will be stored in the `/raw/` directory as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae267c4d-5dd9-406d-bb70-c33b8756f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading customers_csv...\n",
      "Reading employees_csv...\n",
      "Reading orders_csv...\n",
      "Reading order_items_csv...\n",
      "Reading payments_csv...\n",
      "--- Verification of Raw DataFrames ---\n",
      "Schema and preview for 'raw_customers_df':\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+--------------------+-----------+----------+------+\n",
      "|customer_id| full_name|               email|signup_date|     phone|region|\n",
      "+-----------+----------+--------------------+-----------+----------+------+\n",
      "|   CUST1000|Customer 0|dangbaotin0506200...| 2025-04-12|0900770487| North|\n",
      "|   CUST1001|Customer 1|dangbaotin0506200...| 2025-04-13|0900216739|  West|\n",
      "|   CUST1002|Customer 2|customer2@example...| 2025-04-14|0900126225| North|\n",
      "|   CUST1003|Customer 3|customer3@example...| 2025-04-15|0900877572| North|\n",
      "|   CUST1004|Customer 4|customer4@example...| 2025-04-16|0900388389| North|\n",
      "+-----------+----------+--------------------+-----------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema and preview for 'raw_employees_df':\n",
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      " |-- team: string (nullable = true)\n",
      " |-- contact_number: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      "\n",
      "+-----------+--------------+--------------------+----------------+----------+--------------+----------+\n",
      "|employee_id|     full_name|               email|            role|      team|contact_number|start_date|\n",
      "+-----------+--------------+--------------------+----------------+----------+--------------+----------+\n",
      "|     EMP001| Nguyen Van An|dangbaotin0506200...|   Data Engineer| Tech Team|    0901234567|2023-05-15|\n",
      "|     EMP002|   Le Thi Binh|binh.le@yourcompa...| DevOps Engineer| Tech Team|    0912345678|2022-11-20|\n",
      "|     EMP003|Tran Van Cuong|cuong.tran@yourco...| Project Manager|Management|    0987654321|2021-02-10|\n",
      "|     EMP004| Pham Thi Dung|dung.pham@yourcom...|Business Analyst|   Product|    0978123456|2023-08-01|\n",
      "|     EMP005|   Hoang Van E|e.hoang@yourcompa...|       QA Tester| Tech Team|    0934567890|2024-01-22|\n",
      "+-----------+--------------+--------------------+----------------+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema and preview for 'raw_orders_df':\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "|   order_id|customer_id|order_date|   status|channel|total_amount|currency|\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "|ORD134fc859|   CUST1007|2025-03-22|  pending| retail|      406.62|     USD|\n",
      "|ORD7592905b|   CUST1000|2025-03-22|completed| mobile|      217.08|     USD|\n",
      "|ORDe2265d3e|   CUST1002|2025-03-23|completed| retail|       60.08|     USD|\n",
      "|ORDc2036ae3|   CUST1006|2025-03-23|completed| retail|      425.27|     USD|\n",
      "|ORD1faef967|   CUST1009|2025-03-23|cancelled| online|      367.57|     USD|\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema and preview for 'raw_order_items_df':\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price_per_unit: string (nullable = true)\n",
      " |-- discount: string (nullable = true)\n",
      "\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "|order_item_id|   order_id|product_id|product_name|   category|quantity|price_per_unit|discount|\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "| ITEMa3fafcd6|ORD134fc859|   PROD109|  Product 18|      Books|       5|         30.19|    9.33|\n",
      "| ITEM630fb62b|ORD7592905b|   PROD131|  Product 12|   Clothing|       2|         46.63|    5.43|\n",
      "| ITEMfdb8f5ca|ORDe2265d3e|   PROD178|  Product 17|Electronics|       5|         33.44|    6.63|\n",
      "| ITEM6584efc3|ORDe2265d3e|   PROD117|   Product 9|Electronics|       1|         75.53|    1.55|\n",
      "| ITEM4d4af61a|ORDc2036ae3|   PROD177|   Product 7|   Clothing|       2|         70.31|    8.53|\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema and preview for 'raw_payments_df':\n",
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+------------+------+--------------+--------------+\n",
      "| payment_id|   order_id|payment_date|amount|payment_method|payment_status|\n",
      "+-----------+-----------+------------+------+--------------+--------------+\n",
      "|PAYebf3d0ff|ORD134fc859|  2025-03-24|406.62|          cash|     cancelled|\n",
      "|PAY3f9845d0|ORD7592905b|  2025-03-22|217.08|   credit_card|        failed|\n",
      "|PAYbda33d40|ORDe2265d3e|  2025-03-26| 60.08|          cash|     cancelled|\n",
      "|PAY47d6dc51|ORDc2036ae3|  2025-03-23|425.27|          cash|     cancelled|\n",
      "|PAY80e39c5b|ORD1faef967|  2025-03-26|367.57|   credit_card|     completed|\n",
      "+-----------+-----------+------------+------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the base path\n",
    "raw_base_path = \"s3a://raw\"\n",
    "\n",
    "# Reading CSV files into DataFrames\n",
    "print(\"Reading customers_csv...\")\n",
    "raw_customers_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/customers_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"Reading employees_csv...\")\n",
    "raw_employees_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/employees_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"Reading orders_csv...\")\n",
    "raw_orders_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/orders_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"Reading order_items_csv...\")\n",
    "raw_order_items_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/order_items_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"Reading payments_csv...\")\n",
    "raw_payments_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/payments_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"--- Verification of Raw DataFrames ---\")\n",
    "\n",
    "print(\"Schema and preview for 'raw_customers_df':\")\n",
    "raw_customers_df.printSchema()\n",
    "raw_customers_df.show(5)\n",
    "\n",
    "print(\"Schema and preview for 'raw_employees_df':\")\n",
    "raw_employees_df.printSchema()\n",
    "raw_employees_df.show(5)\n",
    "\n",
    "print(\"Schema and preview for 'raw_orders_df':\")\n",
    "raw_orders_df.printSchema()\n",
    "raw_orders_df.show(5)\n",
    "\n",
    "print(\"Schema and preview for 'raw_order_items_df':\")\n",
    "raw_order_items_df.printSchema()\n",
    "raw_order_items_df.show(5)\n",
    "\n",
    "print(\"Schema and preview for 'raw_payments_df':\")\n",
    "raw_payments_df.printSchema()\n",
    "raw_payments_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c2699c1-f00c-4728-94f1-0df2d257fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dangbaotin05062003@gmail.com\n"
     ]
    }
   ],
   "source": [
    "data_engineers_df = raw_employees_df.filter(\n",
    "    (col(\"team\") == \"Tech Team\") & (col(\"role\") == \"Data Engineer\")\n",
    ")\n",
    "\n",
    "TECH_TEAM_EMAILs = [row['email'] for row in data_engineers_df.select(\"email\").collect()]\n",
    "\n",
    "recipient_list = \", \".join(TECH_TEAM_EMAILs)\n",
    "\n",
    "print(recipient_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a488e-2306-4f8f-943c-5874bccb9015",
   "metadata": {},
   "source": [
    "---\n",
    "# Raw → Processed Zone\n",
    "- Read and normalize CSVs into structured DataFrames.\n",
    "- Convert all dates into consistent timestamp format.\n",
    "- Deduplicate based on primary keys (e.g., order_id, order_item_id).\n",
    "- Filter out invalid records:\n",
    "  - Orders with total_amount <= 0\n",
    "  - Payments with status = 'failed' or 'cancelled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca10c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: customers_csv...\n",
      "Writing table: customers_csv...\n",
      "Successfully processed and saved 'customers_csv' as CSV to 's3a://processed/customers_csv'.\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: timestamp (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+----------------------------+-------------------+----------+------+\n",
      "|customer_id|full_name |email                       |signup_date        |phone     |region|\n",
      "+-----------+----------+----------------------------+-------------------+----------+------+\n",
      "|CUST1000   |Customer 0|dangbaotin05062003@gmail.com|2025-04-12 00:00:00|0900770487|North |\n",
      "|CUST1001   |Customer 1|dangbaotin05062003@gmail.com|2025-04-13 00:00:00|0900216739|West  |\n",
      "|CUST1002   |Customer 2|customer2@example.com       |2025-04-14 00:00:00|0900126225|North |\n",
      "|CUST1003   |Customer 3|customer3@example.com       |2025-04-15 00:00:00|0900877572|North |\n",
      "|CUST1004   |Customer 4|customer4@example.com       |2025-04-16 00:00:00|0900388389|North |\n",
      "+-----------+----------+----------------------------+-------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: employees_csv...\n",
      "Writing table: employees_csv...\n",
      "Successfully processed and saved 'employees_csv' as CSV to 's3a://processed/employees_csv'.\n",
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- role: string (nullable = true)\n",
      " |-- team: string (nullable = true)\n",
      " |-- contact_number: string (nullable = true)\n",
      " |-- start_date: timestamp (nullable = true)\n",
      "\n",
      "+-----------+--------------+----------------------------+----------------+----------+--------------+-------------------+\n",
      "|employee_id|full_name     |email                       |role            |team      |contact_number|start_date         |\n",
      "+-----------+--------------+----------------------------+----------------+----------+--------------+-------------------+\n",
      "|EMP001     |Nguyen Van An |dangbaotin05062003@gmail.com|Data Engineer   |Tech Team |0901234567    |2023-05-15 00:00:00|\n",
      "|EMP002     |Le Thi Binh   |binh.le@yourcompany.com     |DevOps Engineer |Tech Team |0912345678    |2022-11-20 00:00:00|\n",
      "|EMP003     |Tran Van Cuong|cuong.tran@yourcompany.com  |Project Manager |Management|0987654321    |2021-02-10 00:00:00|\n",
      "|EMP004     |Pham Thi Dung |dung.pham@yourcompany.com   |Business Analyst|Product   |0978123456    |2023-08-01 00:00:00|\n",
      "|EMP005     |Hoang Van E   |e.hoang@yourcompany.com     |QA Tester       |Tech Team |0934567890    |2024-01-22 00:00:00|\n",
      "+-----------+--------------+----------------------------+----------------+----------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: orders_csv...\n",
      "Writing table: orders_csv...\n",
      "Successfully processed and saved 'orders_csv' as CSV to 's3a://processed/orders_csv'.\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|order_id   |customer_id|order_date         |status   |channel|total_amount|currency|\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|ORD067520f9|CUST1009   |2025-04-16 00:00:00|cancelled|mobile |460.5       |USD     |\n",
      "|ORD07a3d76b|CUST1000   |2025-04-08 00:00:00|completed|online |453.66      |USD     |\n",
      "|ORD0bb1fec7|CUST1009   |2025-04-18 00:00:00|completed|mobile |50.17       |USD     |\n",
      "|ORD11bdc76c|CUST1006   |2025-04-12 00:00:00|completed|retail |236.69      |USD     |\n",
      "|ORD134fc859|CUST1007   |2025-03-22 00:00:00|pending  |retail |406.62      |USD     |\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: order_items_csv...\n",
      "Writing table: order_items_csv...\n",
      "Successfully processed and saved 'order_items_csv' as CSV to 's3a://processed/order_items_csv'.\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "|order_item_id|order_id   |product_id|product_name|category|quantity|price_per_unit|discount|\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "|ITEM028261b8 |ORDe3edf024|PROD128   |Product 1   |Books   |4       |36.18         |8.64    |\n",
      "|ITEM03d04aca |ORD48753936|PROD170   |Product 5   |Clothing|4       |88.68         |4.85    |\n",
      "|ITEM0620f14b |ORD2378b604|PROD134   |Product 17  |Home    |4       |12.64         |0.4     |\n",
      "|ITEM094647fa |ORD2f1ce5d6|PROD139   |Product 8   |Books   |2       |7.32          |2.45    |\n",
      "|ITEM0eaad55a |ORDc14cd0b6|PROD192   |Product 14  |Home    |5       |77.03         |6.91    |\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: payments_csv...\n",
      "Writing table: payments_csv...\n",
      "Successfully processed and saved 'payments_csv' as CSV to 's3a://processed/payments_csv'.\n",
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "|payment_id |order_id   |payment_date       |amount|payment_method|payment_status|\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "|PAY0193719f|ORDdadea663|2025-04-02 00:00:00|80.01 |credit_card   |completed     |\n",
      "|PAY19cf1b42|ORDd3bca08d|2025-04-03 00:00:00|168.84|paypal        |completed     |\n",
      "|PAY218c6eea|ORDb45889b4|2025-04-03 00:00:00|196.99|bank_transfer |completed     |\n",
      "|PAY2a7503da|ORD0bb1fec7|2025-04-20 00:00:00|50.17 |paypal        |completed     |\n",
      "|PAY2ef4cd65|ORD9c9304e7|2025-04-04 00:00:00|66.13 |paypal        |completed     |\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEFINE PATHS & SCHEMAS\n",
    "# =============================================================================\n",
    "raw_base_path = \"s3a://raw\"\n",
    "processed_base_path = \"s3a://processed\"\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField(\"employee_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"role\", StringType(), True),\n",
    "    StructField(\"team\", StringType(), True),\n",
    "    StructField(\"contact_number\", StringType(), True),\n",
    "    StructField(\"start_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True)\n",
    "])\n",
    "\n",
    "order_items_schema = StructType([\n",
    "    StructField(\"order_item_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price_per_unit\", DoubleType(), True),\n",
    "    StructField(\"discount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "payments_schema = StructType([\n",
    "    StructField(\"payment_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_date\", DateType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PRE-PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "# def process_table(table_name, schema, primary_key, date_columns=[], filter_condition=None):\n",
    "#     \"\"\"\n",
    "#     Generic function to read, clean, and write a table.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         print(f\"Processing table: {table_name}...\")\n",
    "        \n",
    "#         # Read from raw zone\n",
    "#         input_path = f\"{raw_base_path}/{table_name}.csv\"\n",
    "#         df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "        \n",
    "#         # Convert date columns to timestamp format\n",
    "#         for date_col in date_columns:\n",
    "#             df = df.withColumn(date_col, to_timestamp(col(date_col)))\n",
    "        \n",
    "#         # Apply filter condition if provided\n",
    "#         if filter_condition is not None:\n",
    "#             df = df.filter(filter_condition)\n",
    "            \n",
    "#         # Deduplicate based on primary key\n",
    "#         df = df.dropDuplicates([primary_key])\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing table {table_name}: {e}\")\n",
    "\n",
    "def read_table(table_name, schema, primary_key, date_columns=[]):\n",
    "    \"\"\"\n",
    "    - Read and normalize CSVs into structured DataFrames.\n",
    "    - Convert all dates into consistent timestamp format.\n",
    "    - Deduplicate based on primary keys (e.g., order_id, order_item_id).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing table: {table_name}...\")\n",
    "        \n",
    "        # Read from raw zone\n",
    "        input_path = f\"{raw_base_path}/{table_name}.csv\"\n",
    "        df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "        \n",
    "        # Convert date columns to timestamp format\n",
    "        for date_col in date_columns:\n",
    "            df = df.withColumn(date_col, to_timestamp(col(date_col)))\n",
    "        \n",
    "        # Deduplicate based on primary key\n",
    "        df = df.dropDuplicates([primary_key])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {e}\")\n",
    "        send_etl_error_alert(tech_email=recipient_list, pipeline_stage=\"Read Raw to Processed\", table_name=table_name, error_message=e)\n",
    "        return None\n",
    "\n",
    "def write_table(table_name, df):\n",
    "    try:\n",
    "        print(f\"Writing table: {table_name}...\")\n",
    "        \n",
    "        # Write to processed zone in CSV format with headers\n",
    "        output_path = f\"{processed_base_path}/{table_name}\"\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        # df.write.mode(\"overwrite\").option(\"header\", \"true\").option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\").csv(output_path)\n",
    "        \n",
    "        print(f\"Successfully processed and saved '{table_name}' as CSV to '{output_path}'.\")\n",
    "        \n",
    "        # For verification, show a few rows\n",
    "        df.printSchema()\n",
    "        df.show(5, truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing table {table_name}: {e}\")\n",
    "        send_etl_error_alert(tech_email=recipient_list, pipeline_stage=\"Write Raw to Processed\", table_name=table_name, error_message=e)\n",
    "\n",
    "### customers_csv\n",
    "processed_customers_df = read_table(table_name=\"customers_csv\", \n",
    "                                    schema=customers_schema, \n",
    "                                    primary_key=\"customer_id\", \n",
    "                                    date_columns=[\"signup_date\"])\n",
    "if processed_customers_df:\n",
    "    write_table(\"customers_csv\", processed_customers_df)\n",
    "\n",
    "\n",
    "### employees_csv\n",
    "processed_employees_df = read_table(table_name=\"employees_csv\", \n",
    "                                    schema=employees_schema, \n",
    "                                    primary_key=\"employee_id\", \n",
    "                                    date_columns=[\"start_date\"])\n",
    "if processed_customers_df:\n",
    "    write_table(\"employees_csv\", processed_employees_df)\n",
    "\n",
    "\n",
    "### orders_csv \n",
    "processed_orders_df = read_table(table_name=\"orders_csv\",\n",
    "                                 schema=orders_schema, \n",
    "                                 primary_key=\"order_id\", \n",
    "                                 date_columns=[\"order_date\"])\n",
    "# processed_orders_df.filter(~processed_orders_df.name.isin([\"Alice\", \"Charlie\"])).show()\n",
    "if processed_orders_df:\n",
    "    write_df = processed_orders_df.filter(\"total_amount > 0\")\n",
    "    write_table(\"orders_csv\", write_df)\n",
    "\n",
    "### order_items_csv\n",
    "processed_order_items_df = read_table(table_name=\"order_items_csv\",\n",
    "                                      schema=order_items_schema,\n",
    "                                      primary_key=\"order_item_id\")\n",
    "if processed_order_items_df:\n",
    "    write_table(\"order_items_csv\", processed_order_items_df)\n",
    "\n",
    "### payments_csv\n",
    "processed_payments_df = read_table(table_name=\"payments_csv\", \n",
    "                                   schema=payments_schema, \n",
    "                                   primary_key=\"payment_id\", \n",
    "                                   date_columns=[\"payment_date\"])\n",
    "if processed_payments_df:\n",
    "    write_df = processed_payments_df.filter(~processed_payments_df.payment_status.isin([\"failed\", \"cancelled\"]))\n",
    "    write_table(\"payments_csv\", write_df)\n",
    "\n",
    "send_etl_success_notification(tech_email=recipient_list, pipeline_stage=\"Raw to Processed\", output_path=processed_base_path)\n",
    "\n",
    "\n",
    "# Process each table according to the requirements\n",
    "# process_table(\n",
    "#     table_name=\"customers_csv\",\n",
    "#     schema=customers_schema,\n",
    "#     primary_key=\"customer_id\",\n",
    "#     date_columns=[\"signup_date\"]\n",
    "# )\n",
    "\n",
    "# process_table(\n",
    "#     table_name=\"orders_csv\",\n",
    "#     schema=orders_schema,\n",
    "#     primary_key=\"order_id\",\n",
    "#     date_columns=[\"order_date\"],\n",
    "#     filter_condition=\"total_amount > 0\"  # Filter out orders with total_amount <= 0\n",
    "# )\n",
    "\n",
    "# process_table(\n",
    "#     table_name=\"order_items_csv\",\n",
    "#     schema=order_items_schema,\n",
    "#     primary_key=\"order_item_id\"\n",
    "# )\n",
    "\n",
    "# process_table(\n",
    "#     table_name=\"payments_csv\",\n",
    "#     schema=payments_schema,\n",
    "#     primary_key=\"payment_id\",\n",
    "#     date_columns=[\"payment_date\"],\n",
    "#     filter_condition=~col(\"payment_status\").isin([\"failed\", \"cancelled\"]) # Filter out failed or cancelled payments\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8472247d-bb43-48a5-942e-98ffe9188b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed customers data:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: timestamp (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+--------------------+-------------------+----------+------+\n",
      "|customer_id| full_name|               email|        signup_date|     phone|region|\n",
      "+-----------+----------+--------------------+-------------------+----------+------+\n",
      "|   CUST1000|Customer 0|dangbaotin0506200...|2025-04-12 00:00:00|0900770487| North|\n",
      "|   CUST1001|Customer 1|dangbaotin0506200...|2025-04-13 00:00:00|0900216739|  West|\n",
      "|   CUST1002|Customer 2|customer2@example...|2025-04-14 00:00:00|0900126225| North|\n",
      "|   CUST1003|Customer 3|customer3@example...|2025-04-15 00:00:00|0900877572| North|\n",
      "|   CUST1004|Customer 4|customer4@example...|2025-04-16 00:00:00|0900388389| North|\n",
      "|   CUST1005|Customer 5|customer5@example...|2025-04-17 00:00:00|0900356787| South|\n",
      "|   CUST1006|Customer 6|dangbaotin0506200...|2025-04-18 00:00:00|0900334053| South|\n",
      "|   CUST1007|Customer 7|customer7@example...|2025-04-19 00:00:00|0900246316| North|\n",
      "|   CUST1008|Customer 8|dangbaotin0506200...|2025-04-20 00:00:00|0900872246| South|\n",
      "|   CUST1009|Customer 9|dangbaotin0506200...|2025-04-21 00:00:00|0900207473|  West|\n",
      "+-----------+----------+--------------------+-------------------+----------+------+\n",
      "\n",
      "Loaded processed orders data:\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+--------+-----------+----------+------+-------+------------+--------+\n",
      "|order_id|customer_id|order_date|status|channel|total_amount|currency|\n",
      "+--------+-----------+----------+------+-------+------------+--------+\n",
      "+--------+-----------+----------+------+-------+------------+--------+\n",
      "\n",
      "Loaded processed order items data:\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "|order_item_id|   order_id|product_id|product_name|   category|quantity|price_per_unit|discount|\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "| ITEM028261b8|ORDe3edf024|   PROD128|   Product 1|      Books|       4|         36.18|    8.64|\n",
      "| ITEM03d04aca|ORD48753936|   PROD170|   Product 5|   Clothing|       4|         88.68|    4.85|\n",
      "| ITEM0620f14b|ORD2378b604|   PROD134|  Product 17|       Home|       4|         12.64|     0.4|\n",
      "| ITEM094647fa|ORD2f1ce5d6|   PROD139|   Product 8|      Books|       2|          7.32|    2.45|\n",
      "| ITEM0eaad55a|ORDc14cd0b6|   PROD192|  Product 14|       Home|       5|         77.03|    6.91|\n",
      "| ITEM1028205c|ORDe67409f2|   PROD196|   Product 8|   Clothing|       5|         51.04|    2.39|\n",
      "| ITEM103fe0e9|ORD1faef967|   PROD194|  Product 15|       Home|       5|          5.92|    0.75|\n",
      "| ITEM11736552|ORD44ceb9eb|   PROD189|   Product 8|       Home|       2|         86.32|    4.11|\n",
      "| ITEM13f22121|ORDc2036ae3|   PROD164|  Product 16|   Clothing|       1|         13.77|    4.24|\n",
      "| ITEM1445acd2|ORD86674fa7|   PROD105|  Product 12|      Books|       2|         68.36|    3.54|\n",
      "| ITEM155b9b73|ORD941cc478|   PROD175|  Product 19|       Home|       2|         47.42|    4.84|\n",
      "| ITEM1591e1ef|ORD75d04717|   PROD145|   Product 8|      Books|       5|         28.81|    7.56|\n",
      "| ITEM17153d0d|ORD6b8e833b|   PROD168|   Product 1|       Home|       5|         58.62|    0.27|\n",
      "| ITEM1785689d|ORD43f40926|   PROD160|   Product 8|      Books|       4|         38.22|    8.21|\n",
      "| ITEM193c0ce0|ORD835276b1|   PROD156|   Product 7|       Home|       2|         67.59|    2.84|\n",
      "| ITEM1a0b6cff|ORD3c493531|   PROD160|  Product 10|Electronics|       2|         32.37|    2.83|\n",
      "| ITEM1c3ce2c3|ORD3c493531|   PROD184|   Product 7|       Home|       1|         56.73|    6.48|\n",
      "| ITEM1cede99c|ORDc54e786b|   PROD196|  Product 15|       Home|       3|          7.73|    8.51|\n",
      "| ITEM20aae417|ORD469940d8|   PROD132|  Product 16|Electronics|       1|         43.07|    0.74|\n",
      "| ITEM22873b0e|ORDdadea663|   PROD164|  Product 14|       Home|       2|         75.65|    4.75|\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Loaded processed payments data:\n",
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n",
      "+----------+--------+------------+------+--------------+--------------+\n",
      "|payment_id|order_id|payment_date|amount|payment_method|payment_status|\n",
      "+----------+--------+------------+------+--------------+--------------+\n",
      "+----------+--------+------------+------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_base_path = \"s3a://processed\"\n",
    "\n",
    "# --- Reading the 'customers' dataset ---\n",
    "# Point Spark to the PARENT DIRECTORY. Spark handles the part-files automatically.\n",
    "# print(\"Loading processed customers data...\")\n",
    "# customers_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/customers_csv\") # Note the path is to the directory\n",
    "\n",
    "# # --- Reading the 'orders' dataset ---\n",
    "# print(\"Loading processed orders data...\")\n",
    "# orders_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(orders_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/orders_csv\")\n",
    "\n",
    "# # --- Reading the 'order_items' dataset ---\n",
    "# print(\"Loading processed order_items data...\")\n",
    "# order_items_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(order_items_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/order_items_csv\")\n",
    "\n",
    "# # --- Reading the 'payments' dataset ---\n",
    "# print(\"Loading processed order_items data...\")\n",
    "# payments_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(payments_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "customers_df = spark.read.parquet(f\"{processed_base_path}/customers_csv\")\n",
    "orders_df = spark.read.parquet(f\"{processed_base_path}/orders_csv\")\n",
    "order_items_df = spark.read.parquet(f\"{processed_base_path}/order_items_csv\")\n",
    "payments_df = spark.read.parquet(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "# Data validate\n",
    "print(\"Loaded processed customers data:\")\n",
    "customers_df.printSchema()\n",
    "customers_df.show()\n",
    "\n",
    "print(\"Loaded processed orders data:\")\n",
    "orders_df.printSchema()\n",
    "orders_df.filter(\"total_amount <= 0\").show()\n",
    "\n",
    "print(\"Loaded processed order items data:\")\n",
    "order_items_df.printSchema()\n",
    "order_items_df.show()\n",
    "\n",
    "print(\"Loaded processed payments data:\")\n",
    "payments_df.printSchema()\n",
    "payments_df.filter(payments_df.payment_status.isin([\"failed\", \"cancelled\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1dc09-a727-4aac-949f-8c87a5705d80",
   "metadata": {},
   "source": [
    "---\n",
    "# Processed → Curated Zone\n",
    "Generate the following curated datasets:\n",
    "\n",
    "## `customer_orders_summary`\n",
    "- Total number of orders per customer\n",
    "- Total amount spent\n",
    "- Average order value\n",
    "- First and last order dates\n",
    "- Customer active status (last order within 90 days)\n",
    "\n",
    "## `order_facts`\n",
    "- Join orders, items, payments\n",
    "- Compute net revenue = (quantity × price - discount)\n",
    "- Enrich with customer and region info\n",
    "\n",
    "## `daily_sales_aggregates`\n",
    "- Group by order_date, region, and channel\n",
    "- Metrics:\n",
    "  - Total sales\n",
    "  - Order count\n",
    "  - Unique customers\n",
    "  - Most used payment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "747d909e-79cb-48ae-a32c-d71aa99a5224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed tables loaded.\n",
      "--- Creating customer_orders_summary ---\n",
      "Created and saved customer_orders_summary.\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "|customer_id|total_orders|total_amount_spent|average_order_value|first_order_date   |last_order_date    |active_status|\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "|CUST1004   |6           |1926.18           |321.03000000000003 |2025-03-26 00:00:00|2025-04-19 00:00:00|inactive     |\n",
      "|CUST1009   |10          |2471.38           |247.138            |2025-03-23 00:00:00|2025-04-18 00:00:00|inactive     |\n",
      "|CUST1006   |8           |1306.81           |163.35125          |2025-03-23 00:00:00|2025-04-18 00:00:00|inactive     |\n",
      "|CUST1005   |1           |231.23            |231.23             |2025-04-06 00:00:00|2025-04-06 00:00:00|inactive     |\n",
      "|CUST1007   |4           |1318.61           |329.6525           |2025-03-22 00:00:00|2025-04-14 00:00:00|inactive     |\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- Creating order_facts ---\n",
      "Created and saved order_facts.\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "|order_date         |order_id   |order_item_id|customer_id|customer_name|region|product_id|product_name|category|quantity|price_per_unit|discount|net_revenue       |channel|\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "|2025-03-25 00:00:00|ORDe3edf024|ITEM028261b8 |CUST1001   |Customer 1   |West  |PROD128   |Product 1   |Books   |4       |36.18         |8.64    |136.07999999999998|retail |\n",
      "|2025-04-09 00:00:00|ORD48753936|ITEM03d04aca |CUST1007   |Customer 7   |North |PROD170   |Product 5   |Clothing|4       |88.68         |4.85    |349.87            |mobile |\n",
      "|2025-04-07 00:00:00|ORD2378b604|ITEM0620f14b |CUST1008   |Customer 8   |South |PROD134   |Product 17  |Home    |4       |12.64         |0.4     |50.160000000000004|mobile |\n",
      "|2025-03-28 00:00:00|ORD2f1ce5d6|ITEM094647fa |CUST1003   |Customer 3   |North |PROD139   |Product 8   |Books   |2       |7.32          |2.45    |12.190000000000001|retail |\n",
      "|2025-04-03 00:00:00|ORDc14cd0b6|ITEM0eaad55a |CUST1002   |Customer 2   |North |PROD192   |Product 14  |Home    |5       |77.03         |6.91    |378.23999999999995|online |\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, count, sum, avg, min, max, date_sub, current_date, when, lit,\n",
    "    row_number, rank, first\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PROCESSED (SILVER) DATA\n",
    "# =============================================================================\n",
    "\n",
    "processed_base_path = \"s3a://processed\"\n",
    "curated_base_path = \"s3a://curated\"\n",
    "\n",
    "# Load the four processed tables\n",
    "customers_df = spark.read.parquet(f\"{processed_base_path}/customers_csv\")\n",
    "orders_df = spark.read.parquet(f\"{processed_base_path}/orders_csv\")\n",
    "order_items_df = spark.read.parquet(f\"{processed_base_path}/order_items_csv\")\n",
    "payments_df = spark.read.parquet(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "# customers_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/customers_csv\")\n",
    "\n",
    "# orders_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/orders_csv\")\n",
    "\n",
    "# order_items_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/order_items_csv\")\n",
    "\n",
    "# payments_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "print(\"All processed tables loaded.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE customer_orders_summary\n",
    "# =============================================================================\n",
    "print(\"--- Creating customer_orders_summary ---\")\n",
    "\n",
    "try:\n",
    "    customer_orders_summary = orders_df.groupBy(\"customer_id\").agg(\n",
    "        count(\"order_id\").alias(\"total_orders\"),\n",
    "        sum(\"total_amount\").alias(\"total_amount_spent\"),\n",
    "        avg(\"total_amount\").alias(\"average_order_value\"),\n",
    "        min(\"order_date\").alias(\"first_order_date\"),\n",
    "        max(\"order_date\").alias(\"last_order_date\")\n",
    "    ).withColumn(\n",
    "        \"active_status\",\n",
    "        when(col(\"last_order_date\") >= date_sub(current_date(), 90), lit(\"active\"))\n",
    "        .otherwise(lit(\"inactive\"))\n",
    "    )\n",
    "    \n",
    "    # Write to the curated zone\n",
    "    customer_orders_summary.write.mode(\"overwrite\").parquet(f\"{curated_base_path}/customer_orders_summary\")\n",
    "    \n",
    "    print(\"Created and saved customer_orders_summary.\")\n",
    "    customer_orders_summary.show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing table customer_orders_summary: {e}\")\n",
    "    send_etl_error_alert(tech_email=recipient_list, pipeline_stage=\"Processed to Curated\", table_name=\"customer_orders_summary\", error_message=e)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE order_facts\n",
    "# =============================================================================\n",
    "print(\"--- Creating order_facts ---\")\n",
    "\n",
    "try:\n",
    "    # Join orders, items, and customer info\n",
    "    order_facts = order_items_df.join(\n",
    "        orders_df,\n",
    "        order_items_df.order_id == orders_df.order_id,\n",
    "        \"inner\"\n",
    "    ).join(\n",
    "        customers_df,\n",
    "        orders_df.customer_id == customers_df.customer_id,\n",
    "        \"inner\"\n",
    "    ).withColumn(\n",
    "        \"net_revenue\",\n",
    "        (col(\"quantity\") * col(\"price_per_unit\")) - col(\"discount\")\n",
    "    ).select(\n",
    "        orders_df[\"order_date\"],\n",
    "        orders_df[\"order_id\"],\n",
    "        order_items_df[\"order_item_id\"],\n",
    "        customers_df[\"customer_id\"],\n",
    "        customers_df[\"full_name\"].alias(\"customer_name\"),\n",
    "        customers_df[\"region\"],\n",
    "        order_items_df[\"product_id\"],\n",
    "        order_items_df[\"product_name\"],\n",
    "        order_items_df[\"category\"],\n",
    "        order_items_df[\"quantity\"],\n",
    "        order_items_df[\"price_per_unit\"],\n",
    "        order_items_df[\"discount\"],\n",
    "        \"net_revenue\",\n",
    "        orders_df[\"channel\"]\n",
    "    )\n",
    "    \n",
    "    # Write to the curated zone, partitioned by order_date\n",
    "    order_facts.write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(f\"{curated_base_path}/order_facts\")\n",
    "    \n",
    "    print(\"Created and saved order_facts.\")\n",
    "    order_facts.show(5, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing table order_facts: {e}\")\n",
    "    send_etl_error_alert(tech_email=recipient_list, pipeline_stage=\"Processed to Curated\", table_name=\"order_facts\", error_message=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e30b2b7-c57b-466d-a90b-e09390a503de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating daily_sales_aggregates ---\n",
      "Created and saved daily_sales_aggregates.\n",
      "+-------------------+------+-------+-----------+-----------+----------------+------------------------+\n",
      "|order_date         |region|channel|total_sales|order_count|unique_customers|most_used_payment_method|\n",
      "+-------------------+------+-------+-----------+-----------+----------------+------------------------+\n",
      "|2025-04-14 00:00:00|North |online |90.67      |1          |1               |cash                    |\n",
      "|2025-04-19 00:00:00|North |retail |74.13      |1          |1               |paypal                  |\n",
      "|2025-04-06 00:00:00|North |retail |205.51     |1          |1               |credit_card             |\n",
      "|2025-03-30 00:00:00|South |retail |80.01      |1          |1               |credit_card             |\n",
      "|2025-04-03 00:00:00|South |mobile |168.84     |1          |1               |paypal                  |\n",
      "|2025-04-12 00:00:00|South |retail |236.69     |1          |1               |paypal                  |\n",
      "|2025-04-01 00:00:00|South |mobile |176.69     |1          |1               |bank_transfer           |\n",
      "|2025-04-04 00:00:00|West  |retail |127.33     |1          |1               |credit_card             |\n",
      "|2025-04-10 00:00:00|North |mobile |492.24     |1          |1               |paypal                  |\n",
      "|2025-03-24 00:00:00|South |mobile |153.66     |1          |1               |paypal                  |\n",
      "+-------------------+------+-------+-----------+-----------+----------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE daily_sales_aggregates\n",
    "# =============================================================================\n",
    "print(\"--- Creating daily_sales_aggregates ---\")\n",
    "\n",
    "try:\n",
    "    # Join the tables\n",
    "    orders_payments_customers_df = orders_df.join(customers_df, \"customer_id\", \"inner\") \\\n",
    "                                            .join(payments_df, \"order_id\", \"inner\")\n",
    "    \n",
    "    # rank payment methods within each group\n",
    "    window_spec = Window.partitionBy(\"order_date\", \"region\", \"channel\").orderBy(col(\"payment_method_count\").desc())\n",
    "    \n",
    "    # count each payment method per group and rank them\n",
    "    most_used_payment_method_df = orders_payments_customers_df.groupBy(\"order_date\", \"region\", \"channel\", \"payment_method\") \\\n",
    "        .count().withColumnRenamed(\"count\", \"payment_method_count\") \\\n",
    "        .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "        .filter(col(\"rank\") == 1) \\\n",
    "        .select(\n",
    "            col(\"order_date\").alias(\"mu_order_date\"),\n",
    "            col(\"region\").alias(\"mu_region\"),\n",
    "            col(\"channel\").alias(\"mu_channel\"),\n",
    "            col(\"payment_method\").alias(\"most_used_payment_method\")\n",
    "        )\n",
    "    \n",
    "    daily_aggregates_df = orders_payments_customers_df.groupBy(\"order_date\", \"region\", \"channel\").agg(\n",
    "        sum(\"total_amount\").alias(\"total_sales\"),\n",
    "        countDistinct(\"order_id\").alias(\"order_count\"),\n",
    "        countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    \n",
    "    # join the aggregates with the most used payment method\n",
    "    daily_sales_aggregates = daily_aggregates_df.join(\n",
    "        most_used_payment_method_df,\n",
    "        (daily_aggregates_df.order_date == most_used_payment_method_df.mu_order_date) &\n",
    "        (daily_aggregates_df.region == most_used_payment_method_df.mu_region) &\n",
    "        (daily_aggregates_df.channel == most_used_payment_method_df.mu_channel),\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        \"order_date\",\n",
    "        \"region\",\n",
    "        \"channel\",\n",
    "        \"total_sales\",\n",
    "        \"order_count\",\n",
    "        \"unique_customers\",\n",
    "        \"most_used_payment_method\"\n",
    "    )\n",
    "    \n",
    "    # write to the curated zone\n",
    "    daily_sales_aggregates.write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(f\"{curated_base_path}/daily_sales_aggregates\")\n",
    "    \n",
    "    print(\"Created and saved daily_sales_aggregates.\")\n",
    "    daily_sales_aggregates.show(10, truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing table daily_sales_aggregates: {e}\")\n",
    "    send_etl_error_alert(tech_email=recipient_list, pipeline_stage=\"Processed to Curated\", table_name=\"daily_sales_aggregates\", error_message=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fa39682-90f8-4830-8de9-8205aec6eb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully sent email to dangbaotin05062003@gmail.com\n"
     ]
    }
   ],
   "source": [
    "send_etl_success_notification(tech_email=recipient_list, pipeline_stage=\"Processed to Curated\", output_path=curated_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973786a-325b-43c6-8306-c9943dd9beb3",
   "metadata": {},
   "source": [
    "---\n",
    "# Advanced Features (Optional)\n",
    "\n",
    "- Use window functions to rank:\n",
    "  - Top 3 customers by revenue in each region\n",
    "  - First-time buyers this week\n",
    "- Add alert for delayed payments (>2 days after order)\n",
    "- Apply SCD Type 2 tracking on customer dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48eb5171-cc1d-401d-8da1-3dbf6f91e57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 3 Customers by Revenue per Region ---\n",
      "Top 3 Customers by Revenue in Each Region:\n",
      "+-----------+-------------+------+------------------+----+\n",
      "|customer_id|customer_name|region|total_revenue     |rank|\n",
      "+-----------+-------------+------+------------------+----+\n",
      "|CUST1003   |Customer 3   |North |2296.3300000000004|1   |\n",
      "|CUST1004   |Customer 4   |North |2209.3700000000003|2   |\n",
      "|CUST1002   |Customer 2   |North |2141.23           |3   |\n",
      "|CUST1008   |Customer 8   |South |2514.05           |1   |\n",
      "|CUST1006   |Customer 6   |South |2156.1599999999994|2   |\n",
      "|CUST1005   |Customer 5   |South |345.43            |3   |\n",
      "|CUST1009   |Customer 9   |West  |3516.83           |1   |\n",
      "|CUST1001   |Customer 1   |West  |2865.44           |2   |\n",
      "+-----------+-------------+------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"--- Top 3 Customers by Revenue per Region ---\")\n",
    "\n",
    "# Calculate total revenue per customer from the order_facts table\n",
    "customer_revenue = order_facts.groupBy(\"customer_id\", \"customer_name\", \"region\") \\\n",
    "    .agg(sum(\"net_revenue\").alias(\"total_revenue\"))\n",
    "\n",
    "# Define the window to partition by region and order by revenue\n",
    "window_spec = Window.partitionBy(\"region\").orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "# Rank customers within each region\n",
    "ranked_customers = customer_revenue.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Filter to get only the top 3 in each region\n",
    "top_3_customers_by_region = ranked_customers.filter(col(\"rank\") <= 3)\n",
    "\n",
    "print(\"Top 3 Customers by Revenue in Each Region:\")\n",
    "top_3_customers_by_region.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0280d2c7-90b2-49b5-bea8-6228aa288d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Identifying First-Time Buyers This Week ---\n",
      "First-Time Buyers Since Column<'date_sub(current_date(), 7)'>:\n",
      "+-----------+----------------+\n",
      "|customer_id|first_order_date|\n",
      "+-----------+----------------+\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_sub, current_date\n",
    "\n",
    "print(\"--- Identifying First-Time Buyers This Week ---\")\n",
    "\n",
    "# Find the first order date for every customer\n",
    "first_order_dates = orders_df.groupBy(\"customer_id\") \\\n",
    "    .agg(min(\"order_date\").alias(\"first_order_date\"))\n",
    "\n",
    "# Define the start of the week (7 days ago)\n",
    "start_of_week = date_sub(current_date(), 7)\n",
    "\n",
    "# Filter for customers has first order in thiz week\n",
    "first_time_buyers_this_week = first_order_dates.filter(\n",
    "    col(\"first_order_date\") >= start_of_week\n",
    ")\n",
    "\n",
    "print(f\"First-Time Buyers Since {start_of_week}:\")\n",
    "first_time_buyers_this_week.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92e13977-7f34-45ed-a774-c741165c7bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Alert for Payments Delayed > 2 Days ---\n",
      "Alerts for Delayed Payments:\n",
      "+-----------+-----------+-------------------+-------------------+-----------+\n",
      "|   order_id|customer_id|         order_date|       payment_date|days_to_pay|\n",
      "+-----------+-----------+-------------------+-------------------+-----------+\n",
      "|ORD11bdc76c|   CUST1006|2025-04-12 00:00:00|2025-04-15 00:00:00|          3|\n",
      "|ORD1faef967|   CUST1009|2025-03-23 00:00:00|2025-03-26 00:00:00|          3|\n",
      "|ORD3c63bceb|   CUST1001|2025-04-04 00:00:00|2025-04-07 00:00:00|          3|\n",
      "|ORD469940d8|   CUST1008|2025-04-06 00:00:00|2025-04-09 00:00:00|          3|\n",
      "|ORD5aa5e9d6|   CUST1009|2025-04-09 00:00:00|2025-04-12 00:00:00|          3|\n",
      "|ORD75d04717|   CUST1008|2025-04-14 00:00:00|2025-04-17 00:00:00|          3|\n",
      "|ORD9c9304e7|   CUST1000|2025-04-01 00:00:00|2025-04-04 00:00:00|          3|\n",
      "|ORDdadea663|   CUST1006|2025-03-30 00:00:00|2025-04-02 00:00:00|          3|\n",
      "+-----------+-----------+-------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "print(\"--- Alert for Payments Delayed > 2 Days ---\")\n",
    "\n",
    "# Join orders and payments tables\n",
    "order_payment_dates = orders_df.join(payments_df, \"order_id\", \"inner\")\n",
    "\n",
    "# Calculate the difference in days\n",
    "payment_delays = order_payment_dates.withColumn(\n",
    "    \"days_to_pay\",\n",
    "    datediff(col(\"payment_date\"), col(\"order_date\"))\n",
    ")\n",
    "\n",
    "# Filter for significant delays\n",
    "delayed_payment_alerts = payment_delays.filter(col(\"days_to_pay\") > 2) \\\n",
    "    .select(\"order_id\", \"customer_id\", \"order_date\", \"payment_date\", \"days_to_pay\")\n",
    "\n",
    "print(\"Alerts for Delayed Payments:\")\n",
    "delayed_payment_alerts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acea860-a314-4f53-b598-6d530d9bba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining delayed payment alerts with customer and order information...\n",
      "+-----------+-----------+-------------------+-------------------+-----------+----------+--------------------+-------------------+----------+------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|   order_id|customer_id|         order_date|       payment_date|days_to_pay| full_name|               email|        signup_date|     phone|region|customer_id|         order_date|   status|channel|total_amount|currency|\n",
      "+-----------+-----------+-------------------+-------------------+-----------+----------+--------------------+-------------------+----------+------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|ORD11bdc76c|   CUST1006|2025-04-12 00:00:00|2025-04-15 00:00:00|          3|Customer 6|dangbaotin0506200...|2025-04-18 00:00:00|0900334053| South|   CUST1006|2025-04-12 00:00:00|completed| retail|      236.69|     USD|\n",
      "|ORD1faef967|   CUST1009|2025-03-23 00:00:00|2025-03-26 00:00:00|          3|Customer 9|dangbaotin0506200...|2025-04-21 00:00:00|0900207473|  West|   CUST1009|2025-03-23 00:00:00|cancelled| online|      367.57|     USD|\n",
      "|ORD3c63bceb|   CUST1001|2025-04-04 00:00:00|2025-04-07 00:00:00|          3|Customer 1|dangbaotin0506200...|2025-04-13 00:00:00|0900216739|  West|   CUST1001|2025-04-04 00:00:00|cancelled| retail|      127.33|     USD|\n",
      "|ORD469940d8|   CUST1008|2025-04-06 00:00:00|2025-04-09 00:00:00|          3|Customer 8|dangbaotin0506200...|2025-04-20 00:00:00|0900872246| South|   CUST1008|2025-04-06 00:00:00|completed| retail|      268.56|     USD|\n",
      "|ORD5aa5e9d6|   CUST1009|2025-04-09 00:00:00|2025-04-12 00:00:00|          3|Customer 9|dangbaotin0506200...|2025-04-21 00:00:00|0900207473|  West|   CUST1009|2025-04-09 00:00:00|  pending| retail|      129.07|     USD|\n",
      "|ORD75d04717|   CUST1008|2025-04-14 00:00:00|2025-04-17 00:00:00|          3|Customer 8|dangbaotin0506200...|2025-04-20 00:00:00|0900872246| South|   CUST1008|2025-04-14 00:00:00|  pending| mobile|      248.47|     USD|\n",
      "|ORD9c9304e7|   CUST1000|2025-04-01 00:00:00|2025-04-04 00:00:00|          3|Customer 0|dangbaotin0506200...|2025-04-12 00:00:00|0900770487| North|   CUST1000|2025-04-01 00:00:00|  pending| mobile|       66.13|     USD|\n",
      "|ORDdadea663|   CUST1006|2025-03-30 00:00:00|2025-04-02 00:00:00|          3|Customer 6|dangbaotin0506200...|2025-04-18 00:00:00|0900334053| South|   CUST1006|2025-03-30 00:00:00|  pending| retail|       80.01|     USD|\n",
      "+-----------+-----------+-------------------+-------------------+-----------+----------+--------------------+-------------------+----------+------+-----------+-------------------+---------+-------+------------+--------+\n",
      "\n",
      "Found 8 delayed payments. Sending alerts...\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n",
      "Successfully sent email to dangbaotin05062003@gmail.com\n",
      "Finished sending all payment alerts.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Enrich the alerts with customer and order details ---\n",
    "\n",
    "print(\"Joining delayed payment alerts with customer and order information...\")\n",
    "\n",
    "# Join alerts with the customers table to get the name and email\n",
    "# Join with the orders table to get the total_amount for the email template\n",
    "alerts_with_customer_info = delayed_payment_alerts.join(\n",
    "    customers_df,\n",
    "    \"customer_id\",\n",
    "    \"inner\"\n",
    ").join(\n",
    "    orders_df,\n",
    "    \"order_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# alerts_with_customer_info.columns()\n",
    "\n",
    "# alerts_with_customer_info = alerts_with_customer_info.select(\n",
    "#     \"full_name\",\n",
    "#     \"email\",\n",
    "#     \"order_id\",\n",
    "#     \"order_date\",\n",
    "#     \"total_amount\"\n",
    "# ).show()\n",
    "\n",
    "# print(\"Successfully enriched alerts. Preparing to send emails...\")\n",
    "alerts_with_customer_info.show()\n",
    "\n",
    "\n",
    "# --- Step 2: Collect the data and loop through to send emails ---\n",
    "\n",
    "# .collect() brings the data from the Spark cluster to the driver node.\n",
    "# Use this only for a small number of alerts to avoid memory issues.\n",
    "alerts_to_send = alerts_with_customer_info.collect()\n",
    "\n",
    "if not alerts_to_send:\n",
    "    print(\"No delayed payment alerts to send.\")\n",
    "else:\n",
    "    print(f\"Found {len(alerts_to_send)} delayed payments. Sending alerts...\")\n",
    "    \n",
    "    # Loop through each row and send an email\n",
    "    for alert in alerts_to_send:\n",
    "        send_delayed_payment_alert(\n",
    "            customer_name=alert[\"full_name\"],\n",
    "            customer_email=alert[\"email\"],\n",
    "            order_id=alert[\"order_id\"],\n",
    "            order_date=str(alert[\"order_date\"].date()), # Format date for readability\n",
    "            order_amount=alert[\"total_amount\"]\n",
    "        )\n",
    "    \n",
    "    print(\"Finished sending all payment alerts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aea657e3-e412-4fbd-833b-1d4ac0c6e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7587837-3779-4daa-a53d-18955c6b4d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
