{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06753bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created and connected to MinIO!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SPARK SESSION INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"COMS_Project_Docker\") \\\n",
    "    .master(\"spark://coms-spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://coms-minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio_user\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio_password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.6,\"\n",
    "        \"org.apache.hadoop:hadoop-client:3.3.6,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.367\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created and connected to MinIO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: customers_csv...\n",
      "Successfully processed and saved 'customers_csv' as CSV to 's3a://processed/customers_csv'.\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: timestamp (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "|customer_id|full_name |email                |signup_date        |phone     |region|\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "|CUST1000   |Customer 0|customer0@example.com|2025-04-12 00:00:00|0900770487|North |\n",
      "|CUST1001   |Customer 1|customer1@example.com|2025-04-13 00:00:00|0900216739|West  |\n",
      "|CUST1002   |Customer 2|customer2@example.com|2025-04-14 00:00:00|0900126225|North |\n",
      "|CUST1003   |Customer 3|customer3@example.com|2025-04-15 00:00:00|0900877572|North |\n",
      "|CUST1004   |Customer 4|customer4@example.com|2025-04-16 00:00:00|0900388389|North |\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: orders_csv...\n",
      "Successfully processed and saved 'orders_csv' as CSV to 's3a://processed/orders_csv'.\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|order_id   |customer_id|order_date         |status   |channel|total_amount|currency|\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|ORD067520f9|CUST1009   |2025-04-16 00:00:00|cancelled|mobile |460.5       |USD     |\n",
      "|ORD07a3d76b|CUST1000   |2025-04-08 00:00:00|completed|online |453.66      |USD     |\n",
      "|ORD0bb1fec7|CUST1009   |2025-04-18 00:00:00|completed|mobile |50.17       |USD     |\n",
      "|ORD11bdc76c|CUST1006   |2025-04-12 00:00:00|completed|retail |236.69      |USD     |\n",
      "|ORD134fc859|CUST1007   |2025-03-22 00:00:00|pending  |retail |406.62      |USD     |\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: order_items_csv...\n",
      "Successfully processed and saved 'order_items_csv' as CSV to 's3a://processed/order_items_csv'.\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "|order_item_id|order_id   |product_id|product_name|category|quantity|price_per_unit|discount|\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "|ITEM028261b8 |ORDe3edf024|PROD128   |Product 1   |Books   |4       |36.18         |8.64    |\n",
      "|ITEM03d04aca |ORD48753936|PROD170   |Product 5   |Clothing|4       |88.68         |4.85    |\n",
      "|ITEM0620f14b |ORD2378b604|PROD134   |Product 17  |Home    |4       |12.64         |0.4     |\n",
      "|ITEM094647fa |ORD2f1ce5d6|PROD139   |Product 8   |Books   |2       |7.32          |2.45    |\n",
      "|ITEM0eaad55a |ORDc14cd0b6|PROD192   |Product 14  |Home    |5       |77.03         |6.91    |\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: payments_csv...\n",
      "Successfully processed and saved 'payments_csv' as CSV to 's3a://processed/payments_csv'.\n",
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "|payment_id |order_id   |payment_date       |amount|payment_method|payment_status|\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "|PAY0193719f|ORDdadea663|2025-04-02 00:00:00|80.01 |credit_card   |completed     |\n",
      "|PAY19cf1b42|ORDd3bca08d|2025-04-03 00:00:00|168.84|paypal        |completed     |\n",
      "|PAY218c6eea|ORDb45889b4|2025-04-03 00:00:00|196.99|bank_transfer |completed     |\n",
      "|PAY2a7503da|ORD0bb1fec7|2025-04-20 00:00:00|50.17 |paypal        |completed     |\n",
      "|PAY2ef4cd65|ORD9c9304e7|2025-04-04 00:00:00|66.13 |paypal        |completed     |\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Bronze to Silver ETL job completed.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. DEFINE PATHS & SCHEMAS\n",
    "# =============================================================================\n",
    "raw_base_path = \"s3a://raw\"\n",
    "processed_base_path = \"s3a://processed\"\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True)\n",
    "])\n",
    "\n",
    "order_items_schema = StructType([\n",
    "    StructField(\"order_item_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price_per_unit\", DoubleType(), True),\n",
    "    StructField(\"discount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "payments_schema = StructType([\n",
    "    StructField(\"payment_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_date\", DateType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. PROCESSING LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "def process_table(table_name, schema, primary_key, date_columns=[], filter_condition=None):\n",
    "    \"\"\"\n",
    "    Generic function to read, clean, and write a table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing table: {table_name}...\")\n",
    "        \n",
    "        # Read from raw zone\n",
    "        input_path = f\"{raw_base_path}/{table_name}.csv\"\n",
    "        df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "        \n",
    "        # Convert date columns to timestamp format\n",
    "        for date_col in date_columns:\n",
    "            df = df.withColumn(date_col, to_timestamp(col(date_col)))\n",
    "        \n",
    "        # Apply filter condition if provided\n",
    "        if filter_condition is not None:\n",
    "            df = df.filter(filter_condition)\n",
    "            \n",
    "        # Deduplicate based on primary key\n",
    "        df = df.dropDuplicates([primary_key])\n",
    "        \n",
    "        # Write to processed zone in CSV format with headers\n",
    "        output_path = f\"{processed_base_path}/{table_name}\"\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        # df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "        \n",
    "        print(f\"Successfully processed and saved '{table_name}' as CSV to '{output_path}'.\")\n",
    "        # For verification, show a few rows\n",
    "        df.printSchema()\n",
    "        df.show(5, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {e}\")\n",
    "\n",
    "# Process each table according to the requirements\n",
    "process_table(\n",
    "    table_name=\"customers_csv\",\n",
    "    schema=customers_schema,\n",
    "    primary_key=\"customer_id\",\n",
    "    date_columns=[\"signup_date\"]\n",
    ")\n",
    "\n",
    "process_table(\n",
    "    table_name=\"orders_csv\",\n",
    "    schema=orders_schema,\n",
    "    primary_key=\"order_id\",\n",
    "    date_columns=[\"order_date\"],\n",
    "    filter_condition=\"total_amount > 0\"  # Filter out orders with total_amount <= 0\n",
    ")\n",
    "\n",
    "process_table(\n",
    "    table_name=\"order_items_csv\",\n",
    "    schema=order_items_schema,\n",
    "    primary_key=\"order_item_id\"\n",
    ")\n",
    "\n",
    "process_table(\n",
    "    table_name=\"payments_csv\",\n",
    "    schema=payments_schema,\n",
    "    primary_key=\"payment_id\",\n",
    "    date_columns=[\"payment_date\"],\n",
    "    filter_condition=~col(\"payment_status\").isin([\"failed\", \"cancelled\"]) # Filter out failed or cancelled payments\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. STOP SPARK SESSION\n",
    "# =============================================================================\n",
    "print(\"Bronze to Silver ETL job completed.\")\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8472247d-bb43-48a5-942e-98ffe9188b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed customers data...\n",
      "Loading processed orders data...\n",
      "Loading processed order_items data...\n",
      "Successfully loaded processed customers data:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+---------------------+-----------+----------+------+\n",
      "|customer_id|full_name |email                |signup_date|phone     |region|\n",
      "+-----------+----------+---------------------+-----------+----------+------+\n",
      "|CUST1000   |Customer 0|customer0@example.com|2025-04-12 |0900770487|North |\n",
      "|CUST1001   |Customer 1|customer1@example.com|2025-04-13 |0900216739|West  |\n",
      "|CUST1002   |Customer 2|customer2@example.com|2025-04-14 |0900126225|North |\n",
      "|CUST1003   |Customer 3|customer3@example.com|2025-04-15 |0900877572|North |\n",
      "|CUST1004   |Customer 4|customer4@example.com|2025-04-16 |0900388389|North |\n",
      "+-----------+----------+---------------------+-----------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Successfully loaded processed orders data:\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "|order_id   |customer_id|order_date|status   |channel|total_amount|currency|\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "|ORD067520f9|CUST1009   |2025-04-16|cancelled|mobile |460.5       |USD     |\n",
      "|ORD07a3d76b|CUST1000   |2025-04-08|completed|online |453.66      |USD     |\n",
      "|ORD0bb1fec7|CUST1009   |2025-04-18|completed|mobile |50.17       |USD     |\n",
      "|ORD11bdc76c|CUST1006   |2025-04-12|completed|retail |236.69      |USD     |\n",
      "|ORD134fc859|CUST1007   |2025-03-22|pending  |retail |406.62      |USD     |\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_base_path = \"s3a://processed\"\n",
    "\n",
    "# --- Reading the 'customers' dataset ---\n",
    "# Point Spark to the PARENT DIRECTORY. Spark handles the part-files automatically.\n",
    "print(\"Loading processed customers data...\")\n",
    "customers_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(customers_schema) \\\n",
    "    .csv(f\"{processed_base_path}/customers_csv\") # Note the path is to the directory\n",
    "\n",
    "# --- Reading the 'orders' dataset ---\n",
    "print(\"Loading processed orders data...\")\n",
    "orders_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(orders_schema) \\\n",
    "    .csv(f\"{processed_base_path}/orders_csv\")\n",
    "\n",
    "# --- Reading the 'order_items' dataset ---\n",
    "print(\"Loading processed order_items data...\")\n",
    "order_items_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(order_items_schema) \\\n",
    "    .csv(f\"{processed_base_path}/order_items_csv\")\n",
    "\n",
    "# Now you can work with these DataFrames\n",
    "print(\"Successfully loaded processed customers data:\")\n",
    "customers_df.printSchema()\n",
    "customers_df.show(5, truncate=False)\n",
    "\n",
    "print(\"Successfully loaded processed orders data:\")\n",
    "orders_df.printSchema()\n",
    "orders_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d909e-79cb-48ae-a32c-d71aa99a5224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
