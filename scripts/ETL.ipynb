{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06753bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created and connected to MinIO!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# =============================================================================\n",
    "# SPARK SESSION INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"COMS_Project_Docker\") \\\n",
    "    .master(\"spark://coms-spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://coms-minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio_user\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio_password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.6,\"\n",
    "        \"org.apache.hadoop:hadoop-client:3.3.6,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.367\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created and connected to MinIO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ea5ba-88e4-4884-ac39-5aebea8c82bb",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Verification\n",
    "Raw input files will be stored in the `/raw/` directory as CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae267c4d-5dd9-406d-bb70-c33b8756f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading customers_csv...\n",
      "Reading orders_csv...\n",
      "Reading order_items_csv...\n",
      "Reading payments_csv...\n",
      "--- Verification of Raw DataFrames ---\n",
      "Schema and preview for 'raw_customers_df':\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+--------------------+-----------+----------+------+\n",
      "|customer_id| full_name|               email|signup_date|     phone|region|\n",
      "+-----------+----------+--------------------+-----------+----------+------+\n",
      "|   CUST1000|Customer 0|customer0@example...| 2025-04-12|0900770487| North|\n",
      "|   CUST1001|Customer 1|customer1@example...| 2025-04-13|0900216739|  West|\n",
      "|   CUST1002|Customer 2|customer2@example...| 2025-04-14|0900126225| North|\n",
      "|   CUST1003|Customer 3|customer3@example...| 2025-04-15|0900877572| North|\n",
      "|   CUST1004|Customer 4|customer4@example...| 2025-04-16|0900388389| North|\n",
      "+-----------+----------+--------------------+-----------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema and preview for 'raw_orders_df':\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "|   order_id|customer_id|order_date|   status|channel|total_amount|currency|\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "|ORD134fc859|   CUST1007|2025-03-22|  pending| retail|      406.62|     USD|\n",
      "|ORD7592905b|   CUST1000|2025-03-22|completed| mobile|      217.08|     USD|\n",
      "|ORDe2265d3e|   CUST1002|2025-03-23|completed| retail|       60.08|     USD|\n",
      "|ORDc2036ae3|   CUST1006|2025-03-23|completed| retail|      425.27|     USD|\n",
      "|ORD1faef967|   CUST1009|2025-03-23|cancelled| online|      367.57|     USD|\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema and preview for 'raw_order_items_df':\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price_per_unit: string (nullable = true)\n",
      " |-- discount: string (nullable = true)\n",
      "\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "|order_item_id|   order_id|product_id|product_name|   category|quantity|price_per_unit|discount|\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "| ITEMa3fafcd6|ORD134fc859|   PROD109|  Product 18|      Books|       5|         30.19|    9.33|\n",
      "| ITEM630fb62b|ORD7592905b|   PROD131|  Product 12|   Clothing|       2|         46.63|    5.43|\n",
      "| ITEMfdb8f5ca|ORDe2265d3e|   PROD178|  Product 17|Electronics|       5|         33.44|    6.63|\n",
      "| ITEM6584efc3|ORDe2265d3e|   PROD117|   Product 9|Electronics|       1|         75.53|    1.55|\n",
      "| ITEM4d4af61a|ORDc2036ae3|   PROD177|   Product 7|   Clothing|       2|         70.31|    8.53|\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema and preview for 'raw_payments_df':\n",
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+------------+------+--------------+--------------+\n",
      "| payment_id|   order_id|payment_date|amount|payment_method|payment_status|\n",
      "+-----------+-----------+------------+------+--------------+--------------+\n",
      "|PAYebf3d0ff|ORD134fc859|  2025-03-24|406.62|          cash|     cancelled|\n",
      "|PAY3f9845d0|ORD7592905b|  2025-03-22|217.08|   credit_card|        failed|\n",
      "|PAYbda33d40|ORDe2265d3e|  2025-03-26| 60.08|          cash|     cancelled|\n",
      "|PAY47d6dc51|ORDc2036ae3|  2025-03-23|425.27|          cash|     cancelled|\n",
      "|PAY80e39c5b|ORD1faef967|  2025-03-26|367.57|   credit_card|     completed|\n",
      "+-----------+-----------+------------+------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the base path\n",
    "raw_base_path = \"s3a://raw\"\n",
    "\n",
    "# Reading CSV files into DataFrames\n",
    "print(\"Reading customers_csv...\")\n",
    "raw_customers_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/customers_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"Reading orders_csv...\")\n",
    "raw_orders_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/orders_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"Reading order_items_csv...\")\n",
    "raw_order_items_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/order_items_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"Reading payments_csv...\")\n",
    "raw_payments_df = spark.read.csv(\n",
    "    f\"{raw_base_path}/payments_csv.csv\",\n",
    "    header=True\n",
    ")\n",
    "\n",
    "print(\"--- Verification of Raw DataFrames ---\")\n",
    "\n",
    "print(\"Schema and preview for 'raw_customers_df':\")\n",
    "raw_customers_df.printSchema()\n",
    "raw_customers_df.show(5)\n",
    "\n",
    "print(\"Schema and preview for 'raw_orders_df':\")\n",
    "raw_orders_df.printSchema()\n",
    "raw_orders_df.show(5)\n",
    "\n",
    "print(\"Schema and preview for 'raw_order_items_df':\")\n",
    "raw_order_items_df.printSchema()\n",
    "raw_order_items_df.show(5)\n",
    "\n",
    "print(\"Schema and preview for 'raw_payments_df':\")\n",
    "raw_payments_df.printSchema()\n",
    "raw_payments_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a488e-2306-4f8f-943c-5874bccb9015",
   "metadata": {},
   "source": [
    "---\n",
    "# Raw → Processed Zone\n",
    "- Read and normalize CSVs into structured DataFrames.\n",
    "- Convert all dates into consistent timestamp format.\n",
    "- Deduplicate based on primary keys (e.g., order_id, order_item_id).\n",
    "- Filter out invalid records:\n",
    "  - Orders with total_amount <= 0\n",
    "  - Payments with status = 'failed' or 'cancelled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca10c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: customers_csv...\n",
      "Writing table: customers_csv...\n",
      "Successfully processed and saved 'customers_csv' as CSV to 's3a://processed/customers_csv'.\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: timestamp (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "|customer_id|full_name |email                |signup_date        |phone     |region|\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "|CUST1000   |Customer 0|customer0@example.com|2025-04-12 00:00:00|0900770487|North |\n",
      "|CUST1001   |Customer 1|customer1@example.com|2025-04-13 00:00:00|0900216739|West  |\n",
      "|CUST1002   |Customer 2|customer2@example.com|2025-04-14 00:00:00|0900126225|North |\n",
      "|CUST1003   |Customer 3|customer3@example.com|2025-04-15 00:00:00|0900877572|North |\n",
      "|CUST1004   |Customer 4|customer4@example.com|2025-04-16 00:00:00|0900388389|North |\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: orders_csv...\n",
      "Writing table: orders_csv...\n",
      "Successfully processed and saved 'orders_csv' as CSV to 's3a://processed/orders_csv'.\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|order_id   |customer_id|order_date         |status   |channel|total_amount|currency|\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|ORD067520f9|CUST1009   |2025-04-16 00:00:00|cancelled|mobile |460.5       |USD     |\n",
      "|ORD07a3d76b|CUST1000   |2025-04-08 00:00:00|completed|online |453.66      |USD     |\n",
      "|ORD0bb1fec7|CUST1009   |2025-04-18 00:00:00|completed|mobile |50.17       |USD     |\n",
      "|ORD11bdc76c|CUST1006   |2025-04-12 00:00:00|completed|retail |236.69      |USD     |\n",
      "|ORD134fc859|CUST1007   |2025-03-22 00:00:00|pending  |retail |406.62      |USD     |\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: order_items_csv...\n",
      "Writing table: order_items_csv...\n",
      "Successfully processed and saved 'order_items_csv' as CSV to 's3a://processed/order_items_csv'.\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "|order_item_id|order_id   |product_id|product_name|category|quantity|price_per_unit|discount|\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "|ITEM028261b8 |ORDe3edf024|PROD128   |Product 1   |Books   |4       |36.18         |8.64    |\n",
      "|ITEM03d04aca |ORD48753936|PROD170   |Product 5   |Clothing|4       |88.68         |4.85    |\n",
      "|ITEM0620f14b |ORD2378b604|PROD134   |Product 17  |Home    |4       |12.64         |0.4     |\n",
      "|ITEM094647fa |ORD2f1ce5d6|PROD139   |Product 8   |Books   |2       |7.32          |2.45    |\n",
      "|ITEM0eaad55a |ORDc14cd0b6|PROD192   |Product 14  |Home    |5       |77.03         |6.91    |\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: payments_csv...\n",
      "Writing table: payments_csv...\n",
      "Successfully processed and saved 'payments_csv' as CSV to 's3a://processed/payments_csv'.\n",
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "|payment_id |order_id   |payment_date       |amount|payment_method|payment_status|\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "|PAY0193719f|ORDdadea663|2025-04-02 00:00:00|80.01 |credit_card   |completed     |\n",
      "|PAY19cf1b42|ORDd3bca08d|2025-04-03 00:00:00|168.84|paypal        |completed     |\n",
      "|PAY218c6eea|ORDb45889b4|2025-04-03 00:00:00|196.99|bank_transfer |completed     |\n",
      "|PAY2a7503da|ORD0bb1fec7|2025-04-20 00:00:00|50.17 |paypal        |completed     |\n",
      "|PAY2ef4cd65|ORD9c9304e7|2025-04-04 00:00:00|66.13 |paypal        |completed     |\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEFINE PATHS & SCHEMAS\n",
    "# =============================================================================\n",
    "raw_base_path = \"s3a://raw\"\n",
    "processed_base_path = \"s3a://processed\"\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True)\n",
    "])\n",
    "\n",
    "order_items_schema = StructType([\n",
    "    StructField(\"order_item_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price_per_unit\", DoubleType(), True),\n",
    "    StructField(\"discount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "payments_schema = StructType([\n",
    "    StructField(\"payment_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_date\", DateType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PRE-PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "# def process_table(table_name, schema, primary_key, date_columns=[], filter_condition=None):\n",
    "#     \"\"\"\n",
    "#     Generic function to read, clean, and write a table.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         print(f\"Processing table: {table_name}...\")\n",
    "        \n",
    "#         # Read from raw zone\n",
    "#         input_path = f\"{raw_base_path}/{table_name}.csv\"\n",
    "#         df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "        \n",
    "#         # Convert date columns to timestamp format\n",
    "#         for date_col in date_columns:\n",
    "#             df = df.withColumn(date_col, to_timestamp(col(date_col)))\n",
    "        \n",
    "#         # Apply filter condition if provided\n",
    "#         if filter_condition is not None:\n",
    "#             df = df.filter(filter_condition)\n",
    "            \n",
    "#         # Deduplicate based on primary key\n",
    "#         df = df.dropDuplicates([primary_key])\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing table {table_name}: {e}\")\n",
    "\n",
    "def read_table(table_name, schema, primary_key, date_columns=[]):\n",
    "    \"\"\"\n",
    "    - Read and normalize CSVs into structured DataFrames.\n",
    "    - Convert all dates into consistent timestamp format.\n",
    "    - Deduplicate based on primary keys (e.g., order_id, order_item_id).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing table: {table_name}...\")\n",
    "        \n",
    "        # Read from raw zone\n",
    "        input_path = f\"{raw_base_path}/{table_name}.csv\"\n",
    "        df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "        \n",
    "        # Convert date columns to timestamp format\n",
    "        for date_col in date_columns:\n",
    "            df = df.withColumn(date_col, to_timestamp(col(date_col)))\n",
    "        \n",
    "        # Deduplicate based on primary key\n",
    "        df = df.dropDuplicates([primary_key])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "def write_table(table_name, df):\n",
    "    try:\n",
    "        print(f\"Writing table: {table_name}...\")\n",
    "        \n",
    "        # Write to processed zone in CSV format with headers\n",
    "        output_path = f\"{processed_base_path}/{table_name}\"\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        # df.write.mode(\"overwrite\").option(\"header\", \"true\").option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\").csv(output_path)\n",
    "        \n",
    "        print(f\"Successfully processed and saved '{table_name}' as CSV to '{output_path}'.\")\n",
    "        \n",
    "        # For verification, show a few rows\n",
    "        df.printSchema()\n",
    "        df.show(5, truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error writing table {table_name}: {e}\")\n",
    "\n",
    "### customers_csv\n",
    "processed_customers_df = read_table(table_name=\"customers_csv\", \n",
    "                                    schema=customers_schema, \n",
    "                                    primary_key=\"customer_id\", \n",
    "                                    date_columns=[\"signup_date\"])\n",
    "if processed_customers_df:\n",
    "    write_table(\"customers_csv\", processed_customers_df)\n",
    "\n",
    "\n",
    "### orders_csv \n",
    "processed_orders_df = read_table(table_name=\"orders_csv\",\n",
    "                                 schema=orders_schema, \n",
    "                                 primary_key=\"order_id\", \n",
    "                                 date_columns=[\"order_date\"])\n",
    "# processed_orders_df.filter(~processed_orders_df.name.isin([\"Alice\", \"Charlie\"])).show()\n",
    "if processed_orders_df:\n",
    "    write_df = processed_orders_df.filter(\"total_amount > 0\")\n",
    "    write_table(\"orders_csv\", write_df)\n",
    "\n",
    "### order_items_csv\n",
    "processed_order_items_df = read_table(table_name=\"order_items_csv\",\n",
    "                                      schema=order_items_schema,\n",
    "                                      primary_key=\"order_item_id\")\n",
    "if processed_order_items_df:\n",
    "    write_table(\"order_items_csv\", processed_order_items_df)\n",
    "\n",
    "### payments_csv\n",
    "processed_payments_df = read_table(table_name=\"payments_csv\", \n",
    "                                   schema=payments_schema, \n",
    "                                   primary_key=\"payment_id\", \n",
    "                                   date_columns=[\"payment_date\"])\n",
    "if processed_payments_df:\n",
    "    write_df = processed_payments_df.filter(~processed_payments_df.payment_status.isin([\"failed\", \"cancelled\"]))\n",
    "    write_table(\"payments_csv\", write_df)\n",
    "\n",
    "\n",
    "# Process each table according to the requirements\n",
    "# process_table(\n",
    "#     table_name=\"customers_csv\",\n",
    "#     schema=customers_schema,\n",
    "#     primary_key=\"customer_id\",\n",
    "#     date_columns=[\"signup_date\"]\n",
    "# )\n",
    "\n",
    "# process_table(\n",
    "#     table_name=\"orders_csv\",\n",
    "#     schema=orders_schema,\n",
    "#     primary_key=\"order_id\",\n",
    "#     date_columns=[\"order_date\"],\n",
    "#     filter_condition=\"total_amount > 0\"  # Filter out orders with total_amount <= 0\n",
    "# )\n",
    "\n",
    "# process_table(\n",
    "#     table_name=\"order_items_csv\",\n",
    "#     schema=order_items_schema,\n",
    "#     primary_key=\"order_item_id\"\n",
    "# )\n",
    "\n",
    "# process_table(\n",
    "#     table_name=\"payments_csv\",\n",
    "#     schema=payments_schema,\n",
    "#     primary_key=\"payment_id\",\n",
    "#     date_columns=[\"payment_date\"],\n",
    "#     filter_condition=~col(\"payment_status\").isin([\"failed\", \"cancelled\"]) # Filter out failed or cancelled payments\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8472247d-bb43-48a5-942e-98ffe9188b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed customers data:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: timestamp (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+--------------------+-------------------+----------+------+\n",
      "|customer_id| full_name|               email|        signup_date|     phone|region|\n",
      "+-----------+----------+--------------------+-------------------+----------+------+\n",
      "|   CUST1000|Customer 0|customer0@example...|2025-04-12 00:00:00|0900770487| North|\n",
      "|   CUST1001|Customer 1|customer1@example...|2025-04-13 00:00:00|0900216739|  West|\n",
      "|   CUST1002|Customer 2|customer2@example...|2025-04-14 00:00:00|0900126225| North|\n",
      "|   CUST1003|Customer 3|customer3@example...|2025-04-15 00:00:00|0900877572| North|\n",
      "|   CUST1004|Customer 4|customer4@example...|2025-04-16 00:00:00|0900388389| North|\n",
      "|   CUST1005|Customer 5|customer5@example...|2025-04-17 00:00:00|0900356787| South|\n",
      "|   CUST1006|Customer 6|customer6@example...|2025-04-18 00:00:00|0900334053| South|\n",
      "|   CUST1007|Customer 7|customer7@example...|2025-04-19 00:00:00|0900246316| North|\n",
      "|   CUST1008|Customer 8|customer8@example...|2025-04-20 00:00:00|0900872246| South|\n",
      "|   CUST1009|Customer 9|customer9@example...|2025-04-21 00:00:00|0900207473|  West|\n",
      "+-----------+----------+--------------------+-------------------+----------+------+\n",
      "\n",
      "Loaded processed orders data:\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+--------+-----------+----------+------+-------+------------+--------+\n",
      "|order_id|customer_id|order_date|status|channel|total_amount|currency|\n",
      "+--------+-----------+----------+------+-------+------------+--------+\n",
      "+--------+-----------+----------+------+-------+------------+--------+\n",
      "\n",
      "Loaded processed order items data:\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "|order_item_id|   order_id|product_id|product_name|   category|quantity|price_per_unit|discount|\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "| ITEM028261b8|ORDe3edf024|   PROD128|   Product 1|      Books|       4|         36.18|    8.64|\n",
      "| ITEM03d04aca|ORD48753936|   PROD170|   Product 5|   Clothing|       4|         88.68|    4.85|\n",
      "| ITEM0620f14b|ORD2378b604|   PROD134|  Product 17|       Home|       4|         12.64|     0.4|\n",
      "| ITEM094647fa|ORD2f1ce5d6|   PROD139|   Product 8|      Books|       2|          7.32|    2.45|\n",
      "| ITEM0eaad55a|ORDc14cd0b6|   PROD192|  Product 14|       Home|       5|         77.03|    6.91|\n",
      "| ITEM1028205c|ORDe67409f2|   PROD196|   Product 8|   Clothing|       5|         51.04|    2.39|\n",
      "| ITEM103fe0e9|ORD1faef967|   PROD194|  Product 15|       Home|       5|          5.92|    0.75|\n",
      "| ITEM11736552|ORD44ceb9eb|   PROD189|   Product 8|       Home|       2|         86.32|    4.11|\n",
      "| ITEM13f22121|ORDc2036ae3|   PROD164|  Product 16|   Clothing|       1|         13.77|    4.24|\n",
      "| ITEM1445acd2|ORD86674fa7|   PROD105|  Product 12|      Books|       2|         68.36|    3.54|\n",
      "| ITEM155b9b73|ORD941cc478|   PROD175|  Product 19|       Home|       2|         47.42|    4.84|\n",
      "| ITEM1591e1ef|ORD75d04717|   PROD145|   Product 8|      Books|       5|         28.81|    7.56|\n",
      "| ITEM17153d0d|ORD6b8e833b|   PROD168|   Product 1|       Home|       5|         58.62|    0.27|\n",
      "| ITEM1785689d|ORD43f40926|   PROD160|   Product 8|      Books|       4|         38.22|    8.21|\n",
      "| ITEM193c0ce0|ORD835276b1|   PROD156|   Product 7|       Home|       2|         67.59|    2.84|\n",
      "| ITEM1a0b6cff|ORD3c493531|   PROD160|  Product 10|Electronics|       2|         32.37|    2.83|\n",
      "| ITEM1c3ce2c3|ORD3c493531|   PROD184|   Product 7|       Home|       1|         56.73|    6.48|\n",
      "| ITEM1cede99c|ORDc54e786b|   PROD196|  Product 15|       Home|       3|          7.73|    8.51|\n",
      "| ITEM20aae417|ORD469940d8|   PROD132|  Product 16|Electronics|       1|         43.07|    0.74|\n",
      "| ITEM22873b0e|ORDdadea663|   PROD164|  Product 14|       Home|       2|         75.65|    4.75|\n",
      "+-------------+-----------+----------+------------+-----------+--------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Loaded processed payments data:\n",
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n",
      "+----------+--------+------------+------+--------------+--------------+\n",
      "|payment_id|order_id|payment_date|amount|payment_method|payment_status|\n",
      "+----------+--------+------------+------+--------------+--------------+\n",
      "+----------+--------+------------+------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_base_path = \"s3a://processed\"\n",
    "\n",
    "# --- Reading the 'customers' dataset ---\n",
    "# Point Spark to the PARENT DIRECTORY. Spark handles the part-files automatically.\n",
    "# print(\"Loading processed customers data...\")\n",
    "# customers_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/customers_csv\") # Note the path is to the directory\n",
    "\n",
    "# # --- Reading the 'orders' dataset ---\n",
    "# print(\"Loading processed orders data...\")\n",
    "# orders_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(orders_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/orders_csv\")\n",
    "\n",
    "# # --- Reading the 'order_items' dataset ---\n",
    "# print(\"Loading processed order_items data...\")\n",
    "# order_items_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(order_items_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/order_items_csv\")\n",
    "\n",
    "# # --- Reading the 'payments' dataset ---\n",
    "# print(\"Loading processed order_items data...\")\n",
    "# payments_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(payments_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "customers_df = spark.read.parquet(f\"{processed_base_path}/customers_csv\")\n",
    "orders_df = spark.read.parquet(f\"{processed_base_path}/orders_csv\")\n",
    "order_items_df = spark.read.parquet(f\"{processed_base_path}/order_items_csv\")\n",
    "payments_df = spark.read.parquet(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "# Data validate\n",
    "print(\"Loaded processed customers data:\")\n",
    "customers_df.printSchema()\n",
    "customers_df.show()\n",
    "\n",
    "print(\"Loaded processed orders data:\")\n",
    "orders_df.printSchema()\n",
    "orders_df.filter(\"total_amount <= 0\").show()\n",
    "\n",
    "print(\"Loaded processed order items data:\")\n",
    "order_items_df.printSchema()\n",
    "order_items_df.show()\n",
    "\n",
    "print(\"Loaded processed payments data:\")\n",
    "payments_df.printSchema()\n",
    "payments_df.filter(payments_df.payment_status.isin([\"failed\", \"cancelled\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1dc09-a727-4aac-949f-8c87a5705d80",
   "metadata": {},
   "source": [
    "---\n",
    "# Processed → Curated Zone\n",
    "Generate the following curated datasets:\n",
    "\n",
    "## `customer_orders_summary`\n",
    "- Total number of orders per customer\n",
    "- Total amount spent\n",
    "- Average order value\n",
    "- First and last order dates\n",
    "- Customer active status (last order within 90 days)\n",
    "\n",
    "## `order_facts`\n",
    "- Join orders, items, payments\n",
    "- Compute net revenue = (quantity × price - discount)\n",
    "- Enrich with customer and region info\n",
    "\n",
    "## `daily_sales_aggregates`\n",
    "- Group by order_date, region, and channel\n",
    "- Metrics:\n",
    "  - Total sales\n",
    "  - Order count\n",
    "  - Unique customers\n",
    "  - Most used payment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "747d909e-79cb-48ae-a32c-d71aa99a5224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed tables loaded.\n",
      "--- Creating customer_orders_summary ---\n",
      "Created and saved customer_orders_summary.\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "|customer_id|total_orders|total_amount_spent|average_order_value|first_order_date   |last_order_date    |active_status|\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "|CUST1004   |6           |1926.18           |321.03000000000003 |2025-03-26 00:00:00|2025-04-19 00:00:00|inactive     |\n",
      "|CUST1009   |10          |2471.38           |247.138            |2025-03-23 00:00:00|2025-04-18 00:00:00|inactive     |\n",
      "|CUST1006   |8           |1306.81           |163.35125          |2025-03-23 00:00:00|2025-04-18 00:00:00|inactive     |\n",
      "|CUST1005   |1           |231.23            |231.23             |2025-04-06 00:00:00|2025-04-06 00:00:00|inactive     |\n",
      "|CUST1007   |4           |1318.61           |329.6525           |2025-03-22 00:00:00|2025-04-14 00:00:00|inactive     |\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- Creating order_facts ---\n",
      "Created and saved order_facts.\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "|order_date         |order_id   |order_item_id|customer_id|customer_name|region|product_id|product_name|category|quantity|price_per_unit|discount|net_revenue       |channel|\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "|2025-03-25 00:00:00|ORDe3edf024|ITEM028261b8 |CUST1001   |Customer 1   |West  |PROD128   |Product 1   |Books   |4       |36.18         |8.64    |136.07999999999998|retail |\n",
      "|2025-04-09 00:00:00|ORD48753936|ITEM03d04aca |CUST1007   |Customer 7   |North |PROD170   |Product 5   |Clothing|4       |88.68         |4.85    |349.87            |mobile |\n",
      "|2025-04-07 00:00:00|ORD2378b604|ITEM0620f14b |CUST1008   |Customer 8   |South |PROD134   |Product 17  |Home    |4       |12.64         |0.4     |50.160000000000004|mobile |\n",
      "|2025-03-28 00:00:00|ORD2f1ce5d6|ITEM094647fa |CUST1003   |Customer 3   |North |PROD139   |Product 8   |Books   |2       |7.32          |2.45    |12.190000000000001|retail |\n",
      "|2025-04-03 00:00:00|ORDc14cd0b6|ITEM0eaad55a |CUST1002   |Customer 2   |North |PROD192   |Product 14  |Home    |5       |77.03         |6.91    |378.23999999999995|online |\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, count, sum, avg, min, max, date_sub, current_date, when, lit,\n",
    "    row_number, rank, first\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD PROCESSED (SILVER) DATA\n",
    "# =============================================================================\n",
    "\n",
    "processed_base_path = \"s3a://processed\"\n",
    "curated_base_path = \"s3a://curated\"\n",
    "\n",
    "# Load the four processed tables\n",
    "customers_df = spark.read.parquet(f\"{processed_base_path}/customers_csv\")\n",
    "orders_df = spark.read.parquet(f\"{processed_base_path}/orders_csv\")\n",
    "order_items_df = spark.read.parquet(f\"{processed_base_path}/order_items_csv\")\n",
    "payments_df = spark.read.parquet(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "# customers_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/customers_csv\")\n",
    "\n",
    "# orders_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/orders_csv\")\n",
    "\n",
    "# order_items_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/order_items_csv\")\n",
    "\n",
    "# payments_df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(customers_schema) \\\n",
    "#     .csv(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "print(\"All processed tables loaded.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE customer_orders_summary\n",
    "# =============================================================================\n",
    "print(\"--- Creating customer_orders_summary ---\")\n",
    "\n",
    "customer_orders_summary = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"order_id\").alias(\"total_orders\"),\n",
    "    sum(\"total_amount\").alias(\"total_amount_spent\"),\n",
    "    avg(\"total_amount\").alias(\"average_order_value\"),\n",
    "    min(\"order_date\").alias(\"first_order_date\"),\n",
    "    max(\"order_date\").alias(\"last_order_date\")\n",
    ").withColumn(\n",
    "    \"active_status\",\n",
    "    when(col(\"last_order_date\") >= date_sub(current_date(), 90), lit(\"active\"))\n",
    "    .otherwise(lit(\"inactive\"))\n",
    ")\n",
    "\n",
    "# Write to the curated zone\n",
    "customer_orders_summary.write.mode(\"overwrite\").parquet(f\"{curated_base_path}/customer_orders_summary\")\n",
    "\n",
    "print(\"Created and saved customer_orders_summary.\")\n",
    "customer_orders_summary.show(5, truncate=False)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE order_facts\n",
    "# =============================================================================\n",
    "print(\"--- Creating order_facts ---\")\n",
    "\n",
    "# Join orders, items, and customer info\n",
    "order_facts = order_items_df.join(\n",
    "    orders_df,\n",
    "    order_items_df.order_id == orders_df.order_id,\n",
    "    \"inner\"\n",
    ").join(\n",
    "    customers_df,\n",
    "    orders_df.customer_id == customers_df.customer_id,\n",
    "    \"inner\"\n",
    ").withColumn(\n",
    "    \"net_revenue\",\n",
    "    (col(\"quantity\") * col(\"price_per_unit\")) - col(\"discount\")\n",
    ").select(\n",
    "    orders_df[\"order_date\"],\n",
    "    orders_df[\"order_id\"],\n",
    "    order_items_df[\"order_item_id\"],\n",
    "    customers_df[\"customer_id\"],\n",
    "    customers_df[\"full_name\"].alias(\"customer_name\"),\n",
    "    customers_df[\"region\"],\n",
    "    order_items_df[\"product_id\"],\n",
    "    order_items_df[\"product_name\"],\n",
    "    order_items_df[\"category\"],\n",
    "    order_items_df[\"quantity\"],\n",
    "    order_items_df[\"price_per_unit\"],\n",
    "    order_items_df[\"discount\"],\n",
    "    \"net_revenue\",\n",
    "    orders_df[\"channel\"]\n",
    ")\n",
    "\n",
    "# Write to the curated zone, partitioned by order_date\n",
    "order_facts.write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(f\"{curated_base_path}/order_facts\")\n",
    "\n",
    "print(\"Created and saved order_facts.\")\n",
    "order_facts.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e30b2b7-c57b-466d-a90b-e09390a503de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating daily_sales_aggregates ---\n",
      "Created and saved daily_sales_aggregates.\n",
      "+-------------------+------+-------+-----------+-----------+----------------+------------------------+\n",
      "|order_date         |region|channel|total_sales|order_count|unique_customers|most_used_payment_method|\n",
      "+-------------------+------+-------+-----------+-----------+----------------+------------------------+\n",
      "|2025-04-14 00:00:00|North |online |90.67      |1          |1               |cash                    |\n",
      "|2025-04-19 00:00:00|North |retail |74.13      |1          |1               |paypal                  |\n",
      "|2025-04-06 00:00:00|North |retail |205.51     |1          |1               |credit_card             |\n",
      "|2025-03-30 00:00:00|South |retail |80.01      |1          |1               |credit_card             |\n",
      "|2025-04-03 00:00:00|South |mobile |168.84     |1          |1               |paypal                  |\n",
      "|2025-04-12 00:00:00|South |retail |236.69     |1          |1               |paypal                  |\n",
      "|2025-04-01 00:00:00|South |mobile |176.69     |1          |1               |bank_transfer           |\n",
      "|2025-04-04 00:00:00|West  |retail |127.33     |1          |1               |credit_card             |\n",
      "|2025-04-10 00:00:00|North |mobile |492.24     |1          |1               |paypal                  |\n",
      "|2025-03-24 00:00:00|South |mobile |153.66     |1          |1               |paypal                  |\n",
      "+-------------------+------+-------+-----------+-----------+----------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE daily_sales_aggregates\n",
    "# =============================================================================\n",
    "print(\"--- Creating daily_sales_aggregates ---\")\n",
    "\n",
    "# Join the tables\n",
    "orders_payments_customers_df = orders_df.join(customers_df, \"customer_id\", \"inner\") \\\n",
    "                                        .join(payments_df, \"order_id\", \"inner\")\n",
    "\n",
    "# rank payment methods within each group\n",
    "window_spec = Window.partitionBy(\"order_date\", \"region\", \"channel\").orderBy(col(\"payment_method_count\").desc())\n",
    "\n",
    "# count each payment method per group and rank them\n",
    "most_used_payment_method_df = orders_payments_customers_df.groupBy(\"order_date\", \"region\", \"channel\", \"payment_method\") \\\n",
    "    .count().withColumnRenamed(\"count\", \"payment_method_count\") \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\n",
    "        col(\"order_date\").alias(\"mu_order_date\"),\n",
    "        col(\"region\").alias(\"mu_region\"),\n",
    "        col(\"channel\").alias(\"mu_channel\"),\n",
    "        col(\"payment_method\").alias(\"most_used_payment_method\")\n",
    "    )\n",
    "\n",
    "daily_aggregates_df = orders_payments_customers_df.groupBy(\"order_date\", \"region\", \"channel\").agg(\n",
    "    sum(\"total_amount\").alias(\"total_sales\"),\n",
    "    countDistinct(\"order_id\").alias(\"order_count\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    ")\n",
    "\n",
    "# join the aggregates with the most used payment method\n",
    "daily_sales_aggregates = daily_aggregates_df.join(\n",
    "    most_used_payment_method_df,\n",
    "    (daily_aggregates_df.order_date == most_used_payment_method_df.mu_order_date) &\n",
    "    (daily_aggregates_df.region == most_used_payment_method_df.mu_region) &\n",
    "    (daily_aggregates_df.channel == most_used_payment_method_df.mu_channel),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    \"order_date\",\n",
    "    \"region\",\n",
    "    \"channel\",\n",
    "    \"total_sales\",\n",
    "    \"order_count\",\n",
    "    \"unique_customers\",\n",
    "    \"most_used_payment_method\"\n",
    ")\n",
    "\n",
    "# write to the curated zone\n",
    "daily_sales_aggregates.write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(f\"{curated_base_path}/daily_sales_aggregates\")\n",
    "\n",
    "print(\"Created and saved daily_sales_aggregates.\")\n",
    "daily_sales_aggregates.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973786a-325b-43c6-8306-c9943dd9beb3",
   "metadata": {},
   "source": [
    "---\n",
    "# Advanced Features (Optional)\n",
    "\n",
    "- Use window functions to rank:\n",
    "  - Top 3 customers by revenue in each region\n",
    "  - First-time buyers this week\n",
    "- Add alert for delayed payments (>2 days after order)\n",
    "- Apply SCD Type 2 tracking on customer dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48eb5171-cc1d-401d-8da1-3dbf6f91e57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 3 Customers by Revenue per Region ---\n",
      "Top 3 Customers by Revenue in Each Region:\n",
      "+-----------+-------------+------+------------------+----+\n",
      "|customer_id|customer_name|region|total_revenue     |rank|\n",
      "+-----------+-------------+------+------------------+----+\n",
      "|CUST1003   |Customer 3   |North |2296.3300000000004|1   |\n",
      "|CUST1004   |Customer 4   |North |2209.3700000000003|2   |\n",
      "|CUST1002   |Customer 2   |North |2141.23           |3   |\n",
      "|CUST1008   |Customer 8   |South |2514.05           |1   |\n",
      "|CUST1006   |Customer 6   |South |2156.1599999999994|2   |\n",
      "|CUST1005   |Customer 5   |South |345.43            |3   |\n",
      "|CUST1009   |Customer 9   |West  |3516.83           |1   |\n",
      "|CUST1001   |Customer 1   |West  |2865.44           |2   |\n",
      "+-----------+-------------+------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"--- Top 3 Customers by Revenue per Region ---\")\n",
    "\n",
    "# Calculate total revenue per customer from the order_facts table\n",
    "customer_revenue = order_facts.groupBy(\"customer_id\", \"customer_name\", \"region\") \\\n",
    "    .agg(sum(\"net_revenue\").alias(\"total_revenue\"))\n",
    "\n",
    "# Define the window to partition by region and order by revenue\n",
    "window_spec = Window.partitionBy(\"region\").orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "# Rank customers within each region\n",
    "ranked_customers = customer_revenue.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "# Filter to get only the top 3 in each region\n",
    "top_3_customers_by_region = ranked_customers.filter(col(\"rank\") <= 3)\n",
    "\n",
    "print(\"Top 3 Customers by Revenue in Each Region:\")\n",
    "top_3_customers_by_region.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0280d2c7-90b2-49b5-bea8-6228aa288d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Identifying First-Time Buyers This Week ---\n",
      "First-Time Buyers Since Column<'date_sub(current_date(), 7)'>:\n",
      "+-----------+----------------+\n",
      "|customer_id|first_order_date|\n",
      "+-----------+----------------+\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_sub, current_date\n",
    "\n",
    "print(\"--- Identifying First-Time Buyers This Week ---\")\n",
    "\n",
    "# Find the first order date for every customer\n",
    "first_order_dates = orders_df.groupBy(\"customer_id\") \\\n",
    "    .agg(min(\"order_date\").alias(\"first_order_date\"))\n",
    "\n",
    "# Define the start of the week (7 days ago)\n",
    "start_of_week = date_sub(current_date(), 7)\n",
    "\n",
    "# Filter for customers has first order in thiz week\n",
    "first_time_buyers_this_week = first_order_dates.filter(\n",
    "    col(\"first_order_date\") >= start_of_week\n",
    ")\n",
    "\n",
    "print(f\"First-Time Buyers Since {start_of_week}:\")\n",
    "first_time_buyers_this_week.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92e13977-7f34-45ed-a774-c741165c7bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Alert for Payments Delayed > 2 Days ---\n",
      "Alerts for Delayed Payments:\n",
      "+-----------+-----------+-------------------+-------------------+-----------+\n",
      "|   order_id|customer_id|         order_date|       payment_date|days_to_pay|\n",
      "+-----------+-----------+-------------------+-------------------+-----------+\n",
      "|ORD11bdc76c|   CUST1006|2025-04-12 00:00:00|2025-04-15 00:00:00|          3|\n",
      "|ORD1faef967|   CUST1009|2025-03-23 00:00:00|2025-03-26 00:00:00|          3|\n",
      "|ORD3c63bceb|   CUST1001|2025-04-04 00:00:00|2025-04-07 00:00:00|          3|\n",
      "|ORD469940d8|   CUST1008|2025-04-06 00:00:00|2025-04-09 00:00:00|          3|\n",
      "|ORD5aa5e9d6|   CUST1009|2025-04-09 00:00:00|2025-04-12 00:00:00|          3|\n",
      "|ORD75d04717|   CUST1008|2025-04-14 00:00:00|2025-04-17 00:00:00|          3|\n",
      "|ORD9c9304e7|   CUST1000|2025-04-01 00:00:00|2025-04-04 00:00:00|          3|\n",
      "|ORDdadea663|   CUST1006|2025-03-30 00:00:00|2025-04-02 00:00:00|          3|\n",
      "+-----------+-----------+-------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "print(\"--- Alert for Payments Delayed > 2 Days ---\")\n",
    "\n",
    "# Join orders and payments tables\n",
    "order_payment_dates = orders_df.join(payments_df, \"order_id\", \"inner\")\n",
    "\n",
    "# Calculate the difference in days\n",
    "payment_delays = order_payment_dates.withColumn(\n",
    "    \"days_to_pay\",\n",
    "    datediff(col(\"payment_date\"), col(\"order_date\"))\n",
    ")\n",
    "\n",
    "# Filter for significant delays\n",
    "delayed_payment_alerts = payment_delays.filter(col(\"days_to_pay\") > 2) \\\n",
    "    .select(\"order_id\", \"customer_id\", \"order_date\", \"payment_date\", \"days_to_pay\")\n",
    "\n",
    "print(\"Alerts for Delayed Payments:\")\n",
    "delayed_payment_alerts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea657e3-e412-4fbd-833b-1d4ac0c6e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
