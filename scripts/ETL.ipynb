{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06753bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created and connected to MinIO!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SPARK SESSION INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"COMS_Project_Docker\") \\\n",
    "    .master(\"spark://coms-spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://coms-minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio_user\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio_password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.6,\"\n",
    "        \"org.apache.hadoop:hadoop-client:3.3.6,\"\n",
    "        \"com.amazonaws:aws-java-sdk-bundle:1.12.367\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created and connected to MinIO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca10c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: customers_csv...\n",
      "Successfully processed and saved 'customers_csv' as CSV to 's3a://processed/customers_csv'.\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: timestamp (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "|customer_id|full_name |email                |signup_date        |phone     |region|\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "|CUST1000   |Customer 0|customer0@example.com|2025-04-12 00:00:00|0900770487|North |\n",
      "|CUST1001   |Customer 1|customer1@example.com|2025-04-13 00:00:00|0900216739|West  |\n",
      "|CUST1002   |Customer 2|customer2@example.com|2025-04-14 00:00:00|0900126225|North |\n",
      "|CUST1003   |Customer 3|customer3@example.com|2025-04-15 00:00:00|0900877572|North |\n",
      "|CUST1004   |Customer 4|customer4@example.com|2025-04-16 00:00:00|0900388389|North |\n",
      "+-----------+----------+---------------------+-------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: orders_csv...\n",
      "Successfully processed and saved 'orders_csv' as CSV to 's3a://processed/orders_csv'.\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|order_id   |customer_id|order_date         |status   |channel|total_amount|currency|\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "|ORD067520f9|CUST1009   |2025-04-16 00:00:00|cancelled|mobile |460.5       |USD     |\n",
      "|ORD07a3d76b|CUST1000   |2025-04-08 00:00:00|completed|online |453.66      |USD     |\n",
      "|ORD0bb1fec7|CUST1009   |2025-04-18 00:00:00|completed|mobile |50.17       |USD     |\n",
      "|ORD11bdc76c|CUST1006   |2025-04-12 00:00:00|completed|retail |236.69      |USD     |\n",
      "|ORD134fc859|CUST1007   |2025-03-22 00:00:00|pending  |retail |406.62      |USD     |\n",
      "+-----------+-----------+-------------------+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: order_items_csv...\n",
      "Successfully processed and saved 'order_items_csv' as CSV to 's3a://processed/order_items_csv'.\n",
      "root\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_per_unit: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      "\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "|order_item_id|order_id   |product_id|product_name|category|quantity|price_per_unit|discount|\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "|ITEM028261b8 |ORDe3edf024|PROD128   |Product 1   |Books   |4       |36.18         |8.64    |\n",
      "|ITEM03d04aca |ORD48753936|PROD170   |Product 5   |Clothing|4       |88.68         |4.85    |\n",
      "|ITEM0620f14b |ORD2378b604|PROD134   |Product 17  |Home    |4       |12.64         |0.4     |\n",
      "|ITEM094647fa |ORD2f1ce5d6|PROD139   |Product 8   |Books   |2       |7.32          |2.45    |\n",
      "|ITEM0eaad55a |ORDc14cd0b6|PROD192   |Product 14  |Home    |5       |77.03         |6.91    |\n",
      "+-------------+-----------+----------+------------+--------+--------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Processing table: payments_csv...\n",
      "Successfully processed and saved 'payments_csv' as CSV to 's3a://processed/payments_csv'.\n",
      "root\n",
      " |-- payment_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "|payment_id |order_id   |payment_date       |amount|payment_method|payment_status|\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "|PAY0193719f|ORDdadea663|2025-04-02 00:00:00|80.01 |credit_card   |completed     |\n",
      "|PAY19cf1b42|ORDd3bca08d|2025-04-03 00:00:00|168.84|paypal        |completed     |\n",
      "|PAY218c6eea|ORDb45889b4|2025-04-03 00:00:00|196.99|bank_transfer |completed     |\n",
      "|PAY2a7503da|ORD0bb1fec7|2025-04-20 00:00:00|50.17 |paypal        |completed     |\n",
      "|PAY2ef4cd65|ORD9c9304e7|2025-04-04 00:00:00|66.13 |paypal        |completed     |\n",
      "+-----------+-----------+-------------------+------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Bronze to Silver ETL job completed.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. DEFINE PATHS & SCHEMAS\n",
    "# =============================================================================\n",
    "raw_base_path = \"s3a://raw\"\n",
    "processed_base_path = \"s3a://processed\"\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"signup_date\", DateType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True)\n",
    "])\n",
    "\n",
    "order_items_schema = StructType([\n",
    "    StructField(\"order_item_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price_per_unit\", DoubleType(), True),\n",
    "    StructField(\"discount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "payments_schema = StructType([\n",
    "    StructField(\"payment_id\", StringType(), False),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"payment_date\", DateType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. PROCESSING LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "def process_table(table_name, schema, primary_key, date_columns=[], filter_condition=None):\n",
    "    \"\"\"\n",
    "    Generic function to read, clean, and write a table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing table: {table_name}...\")\n",
    "        \n",
    "        # Read from raw zone\n",
    "        input_path = f\"{raw_base_path}/{table_name}.csv\"\n",
    "        df = spark.read.csv(input_path, header=True, schema=schema)\n",
    "        \n",
    "        # Convert date columns to timestamp format\n",
    "        for date_col in date_columns:\n",
    "            df = df.withColumn(date_col, to_timestamp(col(date_col)))\n",
    "        \n",
    "        # Apply filter condition if provided\n",
    "        if filter_condition is not None:\n",
    "            df = df.filter(filter_condition)\n",
    "            \n",
    "        # Deduplicate based on primary key\n",
    "        df = df.dropDuplicates([primary_key])\n",
    "        \n",
    "        # Write to processed zone in CSV format with headers\n",
    "        output_path = f\"{processed_base_path}/{table_name}\"\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        # df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "        \n",
    "        print(f\"Successfully processed and saved '{table_name}' as CSV to '{output_path}'.\")\n",
    "        # For verification, show a few rows\n",
    "        df.printSchema()\n",
    "        df.show(5, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {e}\")\n",
    "\n",
    "# Process each table according to the requirements\n",
    "process_table(\n",
    "    table_name=\"customers_csv\",\n",
    "    schema=customers_schema,\n",
    "    primary_key=\"customer_id\",\n",
    "    date_columns=[\"signup_date\"]\n",
    ")\n",
    "\n",
    "process_table(\n",
    "    table_name=\"orders_csv\",\n",
    "    schema=orders_schema,\n",
    "    primary_key=\"order_id\",\n",
    "    date_columns=[\"order_date\"],\n",
    "    filter_condition=\"total_amount > 0\"  # Filter out orders with total_amount <= 0\n",
    ")\n",
    "\n",
    "process_table(\n",
    "    table_name=\"order_items_csv\",\n",
    "    schema=order_items_schema,\n",
    "    primary_key=\"order_item_id\"\n",
    ")\n",
    "\n",
    "process_table(\n",
    "    table_name=\"payments_csv\",\n",
    "    schema=payments_schema,\n",
    "    primary_key=\"payment_id\",\n",
    "    date_columns=[\"payment_date\"],\n",
    "    filter_condition=~col(\"payment_status\").isin([\"failed\", \"cancelled\"]) # Filter out failed or cancelled payments\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. STOP SPARK SESSION\n",
    "# =============================================================================\n",
    "print(\"Bronze to Silver ETL job completed.\")\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8472247d-bb43-48a5-942e-98ffe9188b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed customers data...\n",
      "Loading processed orders data...\n",
      "Loading processed order_items data...\n",
      "Successfully loaded processed customers data:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- signup_date: date (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+-----------+----------+---------------------+-----------+----------+------+\n",
      "|customer_id|full_name |email                |signup_date|phone     |region|\n",
      "+-----------+----------+---------------------+-----------+----------+------+\n",
      "|CUST1000   |Customer 0|customer0@example.com|2025-04-12 |0900770487|North |\n",
      "|CUST1001   |Customer 1|customer1@example.com|2025-04-13 |0900216739|West  |\n",
      "|CUST1002   |Customer 2|customer2@example.com|2025-04-14 |0900126225|North |\n",
      "|CUST1003   |Customer 3|customer3@example.com|2025-04-15 |0900877572|North |\n",
      "|CUST1004   |Customer 4|customer4@example.com|2025-04-16 |0900388389|North |\n",
      "+-----------+----------+---------------------+-----------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Successfully loaded processed orders data:\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- channel: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "|order_id   |customer_id|order_date|status   |channel|total_amount|currency|\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "|ORD067520f9|CUST1009   |2025-04-16|cancelled|mobile |460.5       |USD     |\n",
      "|ORD07a3d76b|CUST1000   |2025-04-08|completed|online |453.66      |USD     |\n",
      "|ORD0bb1fec7|CUST1009   |2025-04-18|completed|mobile |50.17       |USD     |\n",
      "|ORD11bdc76c|CUST1006   |2025-04-12|completed|retail |236.69      |USD     |\n",
      "|ORD134fc859|CUST1007   |2025-03-22|pending  |retail |406.62      |USD     |\n",
      "+-----------+-----------+----------+---------+-------+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_base_path = \"s3a://processed\"\n",
    "\n",
    "# --- Reading the 'customers' dataset ---\n",
    "# Point Spark to the PARENT DIRECTORY. Spark handles the part-files automatically.\n",
    "print(\"Loading processed customers data...\")\n",
    "customers_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(customers_schema) \\\n",
    "    .csv(f\"{processed_base_path}/customers_csv\") # Note the path is to the directory\n",
    "\n",
    "# --- Reading the 'orders' dataset ---\n",
    "print(\"Loading processed orders data...\")\n",
    "orders_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(orders_schema) \\\n",
    "    .csv(f\"{processed_base_path}/orders_csv\")\n",
    "\n",
    "# --- Reading the 'order_items' dataset ---\n",
    "print(\"Loading processed order_items data...\")\n",
    "order_items_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(order_items_schema) \\\n",
    "    .csv(f\"{processed_base_path}/order_items_csv\")\n",
    "\n",
    "# Now you can work with these DataFrames\n",
    "print(\"Successfully loaded processed customers data:\")\n",
    "customers_df.printSchema()\n",
    "customers_df.show(5, truncate=False)\n",
    "\n",
    "print(\"Successfully loaded processed orders data:\")\n",
    "orders_df.printSchema()\n",
    "orders_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "747d909e-79cb-48ae-a32c-d71aa99a5224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Processed (Silver) Data ---\n",
      "All processed tables loaded successfully.\n",
      "\n",
      "--- Creating 'customer_orders_summary' ---\n",
      "Successfully created and saved 'customer_orders_summary'.\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "|customer_id|total_orders|total_amount_spent|average_order_value|first_order_date   |last_order_date    |active_status|\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "|CUST1004   |6           |1926.18           |321.03000000000003 |2025-03-26 00:00:00|2025-04-19 00:00:00|active       |\n",
      "|CUST1009   |10          |2471.38           |247.138            |2025-03-23 00:00:00|2025-04-18 00:00:00|active       |\n",
      "|CUST1006   |8           |1306.81           |163.35125          |2025-03-23 00:00:00|2025-04-18 00:00:00|active       |\n",
      "|CUST1005   |1           |231.23            |231.23             |2025-04-06 00:00:00|2025-04-06 00:00:00|active       |\n",
      "|CUST1007   |4           |1318.61           |329.6525           |2025-03-22 00:00:00|2025-04-14 00:00:00|active       |\n",
      "+-----------+------------+------------------+-------------------+-------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--- Creating 'order_facts' ---\n",
      "Successfully created and saved 'order_facts'.\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "|order_date         |order_id   |order_item_id|customer_id|customer_name|region|product_id|product_name|category|quantity|price_per_unit|discount|net_revenue       |channel|\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "|2025-03-25 00:00:00|ORDe3edf024|ITEM028261b8 |CUST1001   |Customer 1   |West  |PROD128   |Product 1   |Books   |4       |36.18         |8.64    |136.07999999999998|retail |\n",
      "|2025-04-09 00:00:00|ORD48753936|ITEM03d04aca |CUST1007   |Customer 7   |North |PROD170   |Product 5   |Clothing|4       |88.68         |4.85    |349.87            |mobile |\n",
      "|2025-04-07 00:00:00|ORD2378b604|ITEM0620f14b |CUST1008   |Customer 8   |South |PROD134   |Product 17  |Home    |4       |12.64         |0.4     |50.160000000000004|mobile |\n",
      "|2025-03-28 00:00:00|ORD2f1ce5d6|ITEM094647fa |CUST1003   |Customer 3   |North |PROD139   |Product 8   |Books   |2       |7.32          |2.45    |12.190000000000001|retail |\n",
      "|2025-04-03 00:00:00|ORDc14cd0b6|ITEM0eaad55a |CUST1002   |Customer 2   |North |PROD192   |Product 14  |Home    |5       |77.03         |6.91    |378.23999999999995|online |\n",
      "+-------------------+-----------+-------------+-----------+-------------+------+----------+------------+--------+--------+--------------+--------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, count, sum, avg, min, max, date_sub, current_date, when, lit,\n",
    "    row_number, rank, first\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD PROCESSED (SILVER) DATA\n",
    "# =============================================================================\n",
    "print(\"--- Loading Processed (Silver) Data ---\")\n",
    "\n",
    "processed_base_path = \"s3a://processed\"\n",
    "curated_base_path = \"s3a://curated\"\n",
    "\n",
    "# Load the four processed tables\n",
    "customers_df = spark.read.parquet(f\"{processed_base_path}/customers_csv\")\n",
    "orders_df = spark.read.parquet(f\"{processed_base_path}/orders_csv\")\n",
    "order_items_df = spark.read.parquet(f\"{processed_base_path}/order_items_csv\")\n",
    "payments_df = spark.read.parquet(f\"{processed_base_path}/payments_csv\")\n",
    "\n",
    "print(\"All processed tables loaded successfully.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CREATE `customer_orders_summary`\n",
    "# =============================================================================\n",
    "print(\"\\n--- Creating 'customer_orders_summary' ---\")\n",
    "\n",
    "customer_orders_summary = orders_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"order_id\").alias(\"total_orders\"),\n",
    "    sum(\"total_amount\").alias(\"total_amount_spent\"),\n",
    "    avg(\"total_amount\").alias(\"average_order_value\"),\n",
    "    min(\"order_date\").alias(\"first_order_date\"),\n",
    "    max(\"order_date\").alias(\"last_order_date\")\n",
    ").withColumn(\n",
    "    \"active_status\",\n",
    "    when(col(\"last_order_date\") >= date_sub(current_date(), 90), lit(\"active\"))\n",
    "    .otherwise(lit(\"inactive\"))\n",
    ")\n",
    "\n",
    "# Write to the curated zone\n",
    "customer_orders_summary.write.mode(\"overwrite\").parquet(f\"{curated_base_path}/customer_orders_summary\")\n",
    "\n",
    "print(\"Successfully created and saved 'customer_orders_summary'.\")\n",
    "customer_orders_summary.show(5, truncate=False)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. CREATE `order_facts`\n",
    "# =============================================================================\n",
    "print(\"\\n--- Creating 'order_facts' ---\")\n",
    "\n",
    "# Join orders, items, and customer info\n",
    "order_facts = order_items_df.join(\n",
    "    orders_df,\n",
    "    order_items_df.order_id == orders_df.order_id,\n",
    "    \"inner\"\n",
    ").join(\n",
    "    customers_df,\n",
    "    orders_df.customer_id == customers_df.customer_id,\n",
    "    \"inner\"\n",
    ").withColumn(\n",
    "    \"net_revenue\",\n",
    "    (col(\"quantity\") * col(\"price_per_unit\")) - col(\"discount\")\n",
    ").select(\n",
    "    orders_df[\"order_date\"],\n",
    "    orders_df[\"order_id\"],\n",
    "    order_items_df[\"order_item_id\"],\n",
    "    customers_df[\"customer_id\"],\n",
    "    customers_df[\"full_name\"].alias(\"customer_name\"),\n",
    "    customers_df[\"region\"],\n",
    "    order_items_df[\"product_id\"],\n",
    "    order_items_df[\"product_name\"],\n",
    "    order_items_df[\"category\"],\n",
    "    order_items_df[\"quantity\"],\n",
    "    order_items_df[\"price_per_unit\"],\n",
    "    order_items_df[\"discount\"],\n",
    "    \"net_revenue\",\n",
    "    orders_df[\"channel\"]\n",
    ")\n",
    "\n",
    "# Write to the curated zone, partitioned by order_date\n",
    "order_facts.write.mode(\"overwrite\").partitionBy(\"order_date\").parquet(f\"{curated_base_path}/order_facts\")\n",
    "\n",
    "print(\"Successfully created and saved 'order_facts'.\")\n",
    "order_facts.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30b2b7-c57b-466d-a90b-e09390a503de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
